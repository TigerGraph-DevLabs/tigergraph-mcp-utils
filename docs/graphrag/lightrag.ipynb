{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a421f5f-208d-47ef-9b3c-a1cad761261a",
   "metadata": {},
   "source": [
    "[LightRAG](https://github.com/HKUDS/LightRAG) is an open-source RAG system that enhances LLMs by integrating graph-based structures into text indexing and retrieval. It overcomes the limitations of traditional RAG systems, such as fragmented answers and weak contextual awareness, by enabling dual-level retrieval for more comprehensive knowledge discovery. With support for incremental data updates, LightRAG ensures timely integration of new information while delivering improved retrieval accuracy and efficiency.\n",
    "\n",
    "To run this Jupyter Notebook, you can download the original `.ipynb` file from [lightrag.ipynb](https://github.com/xuanleilin/tigergraphx/tree/main/docs/graphrag/lightrag.ipynb).\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before proceeding, ensure you’ve completed the installation and setup steps outlined in the [Installation Guide](../getting_started/installation.md), including:\n",
    "\n",
    "- Setting up Python and TigerGraph. For more details, refer to the [Requirements](../../getting_started/installation/#requirements) section.\n",
    "- Install TigerGraphX along with its development dependencies. For more details, refer to the [Development Installation](../../getting_started/installation/#development-installation) section.\n",
    "- Set the environment variables **`TG_HOST`**, **`TG_USERNAME`**, and **`TG_PASSWORD`**, which are required to connect to the TigerGraph server, as well as **`OPENAI_API_KEY`** for connecting to OpenAI. Use a command like the following to set these variables:  \n",
    "\n",
    "   ```bash\n",
    "   export TG_HOST=https://127.0.0.1\n",
    "   ```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Implement Graph Storage with TigerGraphX\n",
    "\n",
    "In LightRAG, the storage layers are abstracted into components such as graph storage, key-value storage, and vector storage. You can refer to the base classes **BaseGraphStorage**, **BaseVectorStorage**, and **BaseKVStorage** in the [source code](https://github.com/HKUDS/LightRAG/blob/main/lightrag/base.py).  \n",
    "\n",
    "In this section, we will demonstrate how to use TigerGraphX to implement the `BaseGraphStorage` class for storing and retrieving data in TigerGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4105c86b-0a7b-4e6d-8f22-ac72b7f3b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "from lightrag.base import BaseGraphStorage\n",
    "from lightrag.utils import logger\n",
    "from tigergraphx import UndiGraph, TigerGraphConnectionConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TigerGraphStorage(BaseGraphStorage):\n",
    "    def __post_init__(self):\n",
    "        try:\n",
    "            # Retrieve connection configuration from environment variables\n",
    "            connection_config = {\n",
    "                \"host\": os.environ[\"TG_HOST\"],\n",
    "                \"username\": os.environ[\"TG_USERNAME\"],\n",
    "                \"password\": os.environ[\"TG_PASSWORD\"],\n",
    "            }\n",
    "            logger.info(\"TigerGraph connection configuration retrieved successfully.\")\n",
    "            # Initialize the graph\n",
    "            self._graph = UndiGraph(\n",
    "                graph_name=\"LightRAG\",\n",
    "                node_type=\"MyNode\",\n",
    "                edge_type=\"MyEdge\",\n",
    "                node_primary_key=\"id\",\n",
    "                node_attributes={\n",
    "                    \"id\": \"STRING\",\n",
    "                    \"entity_type\": \"STRING\",\n",
    "                    \"description\": \"STRING\",\n",
    "                    \"source_id\": \"STRING\",\n",
    "                },\n",
    "                edge_attributes={\n",
    "                    \"weight\": \"DOUBLE\",\n",
    "                    \"description\": \"STRING\",\n",
    "                    \"keywords\": \"STRING\",\n",
    "                    \"source_id\": \"STRING\",\n",
    "                },\n",
    "                tigergraph_connection_config=TigerGraphConnectionConfig.ensure_config(\n",
    "                    connection_config\n",
    "                ),\n",
    "            )\n",
    "            logger.info(\n",
    "                \"Undirected graph initialized successfully with graph_name 'LightRAG'.\"\n",
    "            )\n",
    "        except KeyError as e:\n",
    "            logger.error(f\"Environment variable {str(e)} is missing.\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_quotes(value: str) -> str:\n",
    "        \"\"\"Remove leading and trailing &quot; from a string if present.\"\"\"\n",
    "        if value.startswith('\"') and value.endswith('\"'):\n",
    "            return value[1:-1]\n",
    "        return value\n",
    "\n",
    "    async def has_node(self, node_id: str) -> bool:\n",
    "        return self._graph.has_node(self.clean_quotes(node_id))\n",
    "\n",
    "    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:\n",
    "        return self._graph.has_edge(\n",
    "            self.clean_quotes(source_node_id), self.clean_quotes(target_node_id)\n",
    "        )\n",
    "\n",
    "    async def node_degree(self, node_id: str) -> int:\n",
    "        result = self._graph.degree(self.clean_quotes(node_id))\n",
    "        return result\n",
    "\n",
    "    async def edge_degree(self, src_id: str, tgt_id: str) -> int:\n",
    "        return self._graph.degree(self.clean_quotes(src_id)) + self._graph.degree(\n",
    "            self.clean_quotes(tgt_id)\n",
    "        )\n",
    "\n",
    "    async def get_node(self, node_id: str) -> dict | None:\n",
    "        result = self._graph.get_node_data(self.clean_quotes(node_id))\n",
    "        return result\n",
    "\n",
    "    async def get_edge(self, source_node_id: str, target_node_id: str) -> dict | None:\n",
    "        result = self._graph.get_edge_data(\n",
    "            self.clean_quotes(source_node_id), self.clean_quotes(target_node_id)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    async def get_node_edges(self, source_node_id: str) -> list[tuple[str, str]] | None:\n",
    "        source_node_id = self.clean_quotes(source_node_id)\n",
    "        if self._graph.has_node(source_node_id):\n",
    "            edges = self._graph.get_node_edges(source_node_id)\n",
    "            return list(edges)\n",
    "        return None\n",
    "\n",
    "    async def upsert_node(self, node_id: str, node_data: dict[str, str]):\n",
    "        node_id = self.clean_quotes(node_id)\n",
    "        self._graph.add_node(node_id, **node_data)\n",
    "\n",
    "    async def upsert_edge(\n",
    "        self, source_node_id: str, target_node_id: str, edge_data: dict[str, str]\n",
    "    ):\n",
    "        source_node_id = self.clean_quotes(source_node_id)\n",
    "        target_node_id = self.clean_quotes(target_node_id)\n",
    "        self._graph.add_edge(source_node_id, target_node_id, **edge_data)\n",
    "\n",
    "    async def delete_node(self, node_id: str):\n",
    "        if self._graph.has_node(node_id):\n",
    "            self._graph.remove_node(node_id)\n",
    "            logger.info(f\"Node {node_id} deleted from the graph.\")\n",
    "        else:\n",
    "            logger.warning(f\"Node {node_id} not found in the graph for deletion.\")\n",
    "\n",
    "    async def embed_nodes(self, algorithm: str) -> tuple[np.ndarray, list[str]]:\n",
    "        return np.array([]), []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffac3d-df08-4a5e-ad8a-d33b83b58c64",
   "metadata": {},
   "source": [
    "This code defines a `TigerGraphStorage` class that implements the `BaseGraphStorage` interface for graph storage and retrieval using **TigerGraphX**, a Python library for interacting with TigerGraph databases.\n",
    "\n",
    "Key highlights of this implementation include:\n",
    "\n",
    "1. **Graph Initialization**  \n",
    "   - An **undirected homogeneous graph** (`UndiGraph`) is initialized. This graph type supports only one type of node and edge, making it similar to **NetworkX**'s undirected graph.\n",
    "   - TigerGraph’s schema-based nature requires a graph schema definition with specific attributes for nodes and edges. For instance:  \n",
    "     - Node attributes: `id`, `entity_type`, `description`, `source_id`  \n",
    "     - Edge attributes: `weight`, `description`, `keywords`, `source_id`\n",
    "\n",
    "2. **TigerGraphX Interfaces**  \n",
    "   - TigerGraphX provides user-friendly interfaces, very similar to NetworkX, which simplify operations like:  \n",
    "     - **Node Operations**: `has_node`, `add_node`, `remove_node`, `get_node_data`  \n",
    "     - **Edge Operations**: `has_edge`, `add_edge`, `get_edge_data`, `get_node_edges`  \n",
    "     - **Degree Calculation**: `degree` for nodes and edges.\n",
    "\n",
    "3. **Key Methods**  \n",
    "   - **Storage Operations**:  \n",
    "     - `upsert_node`: Inserts or updates a node with its data.  \n",
    "     - `upsert_edge`: Inserts or updates an edge between two nodes.  \n",
    "     - `delete_node`: Deletes a node if it exists.  \n",
    "   - **Data Retrieval**:  \n",
    "     - `get_node`: Retrieves data for a specific node.  \n",
    "     - `get_edge`: Retrieves data for a specific edge.  \n",
    "     - `get_node_edges`: Retrieves all edges for a given node.  \n",
    "   - **Graph Metrics**:  \n",
    "     - `node_degree`: Returns the degree of a node.  \n",
    "     - `edge_degree`: Calculates combined degrees of two nodes.  \n",
    "\n",
    "4. **Additional Notes**  \n",
    "   - The `clean_quotes` method ensures clean input values by stripping leading and trailing quotes from strings.  \n",
    "   - TigerGraphX goes beyond NetworkX’s capabilities by supporting **heterogeneous graphs** (graphs with multiple types of nodes and edges) using the `Graph` class, in addition to undirected (`UndiGraph`) and directed graphs (`DiGraph`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09d175-8ab3-40bc-b969-793f94b6de4b",
   "metadata": {},
   "source": [
    "---\n",
    "## Integrating Custom Graph Storage with LightRAG\n",
    "After defining the `TigerGraphStorage` class, we integrate it into LightRAG. By subclassing LightRAG and extending its storage mapping, you can easily replace or augment the default storage backends with your custom solution.  \n",
    "\n",
    "While modifying the LightRAG source code is another option, this example demonstrates how to achieve the integration without altering the original source code.\n",
    "\n",
    "Below is the code for creating a `CustomLightRAG` class that incorporates `TigerGraphStorage` into its storage mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68abb81-5185-4219-8e02-7a4e980675bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightrag import LightRAG\n",
    "\n",
    "\n",
    "# Define a subclass to include your custom graph storage in the storage mapping\n",
    "class CustomLightRAG(LightRAG):\n",
    "    def _get_storage_class(self):\n",
    "        # Extend the default storage mapping with your custom storage\n",
    "        base_mapping = super()._get_storage_class()\n",
    "        base_mapping[\"TigerGraphStorage\"] = TigerGraphStorage\n",
    "        return base_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446f6b0-2f2a-47e4-a161-c24c571ef06e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Indexing\n",
    "### Data Preparation\n",
    "#### Set Up Working Directory\n",
    "Create a folder to serve as the working directory. For this demo, we will use `applications/lightrag/data`.\n",
    "\n",
    "Next, create an `input` folder inside the `data` directory to store the documents you want to index:  \n",
    "\n",
    "```bash\n",
    "mkdir -p applications/lightrag/data/input\n",
    "```\n",
    "\n",
    "#### Add Documents to the Input Folder\n",
    "Copy your documents (e.g., `fin.txt`) into the `applications/lightrag/data/input` folder.\n",
    "\n",
    "---\n",
    "\n",
    "### Indexing\n",
    "The following code sets up a working directory and demonstrates how to index a given document using LightRAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd5da89-f256-40dd-ba5f-52a3bfbb59e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:20,312 - lightrag - INFO - Logger initialized for working directory: ../../applications/lightrag/data\n",
      "2024-12-17 22:21:20,315 - lightrag - INFO - Load KV llm_response_cache with 0 data\n",
      "2024-12-17 22:21:20,318 - lightrag - INFO - Load KV full_docs with 0 data\n",
      "2024-12-17 22:21:20,320 - lightrag - INFO - Load KV text_chunks with 0 data\n",
      "2024-12-17 22:21:20,322 - lightrag - INFO - TigerGraph connection configuration retrieved successfully.\n",
      "2024-12-17 22:21:20,386 - lightrag - INFO - Undirected graph initialized successfully with graph_name 'LightRAG'.\n",
      "2024-12-17 22:21:20,387 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_entities.json'} 0 data\n",
      "2024-12-17 22:21:20,389 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_relationships.json'} 0 data\n",
      "2024-12-17 22:21:20,390 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_chunks.json'} 0 data\n",
      "2024-12-17 22:21:20,401 - lightrag - INFO - [New Docs] inserting 1 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.96doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:20,456 - lightrag - INFO - [New Chunks] inserting 46 chunks\n",
      "2024-12-17 22:21:20,457 - lightrag - INFO - Inserting 46 vectors to chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                         | 0/2 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:21,645 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:21,957 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.04batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:22,587 - lightrag - INFO - [Entity Extraction]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "enerating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.07s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:31,254 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:32,446 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:33,175 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:33,260 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:33,452 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:33,519 - openai._base_client - INFO - Retrying request to /chat/completions in 0.432498 seconds\n",
      "2024-12-17 22:21:33,881 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:34,010 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:34,094 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:34,153 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:34,437 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:35,645 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:36,193 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:36,368 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:37,039 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:37,099 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:39,156 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:39,479 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠙ Processed 1 chunks, 18 entities(duplicated), 5 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:   2%|██▏                                                                                                   | 1/46 [00:16<12:41, 16.91s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:44,110 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠹ Processed 2 chunks, 35 entities(duplicated), 11 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:   4%|████▍                                                                                                 | 2/46 [00:21<07:06,  9.69s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:46,600 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠸ Processed 3 chunks, 58 entities(duplicated), 11 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:   7%|██████▋                                                                                               | 3/46 [00:24<04:35,  6.40s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:47,774 - openai._base_client - INFO - Retrying request to /chat/completions in 0.417796 seconds\n",
      "2024-12-17 22:21:48,376 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠼ Processed 4 chunks, 79 entities(duplicated), 15 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:   9%|████████▊                                                                                             | 4/46 [00:25<03:12,  4.58s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:48,597 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠴ Processed 5 chunks, 107 entities(duplicated), 21 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  11%|███████████                                                                                           | 5/46 [00:26<02:03,  3.00s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:49,309 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠦ Processed 6 chunks, 125 entities(duplicated), 33 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  13%|█████████████▎                                                                                        | 6/46 [00:26<01:29,  2.23s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:49,332 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:50,339 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠇ Processed 8 chunks, 173 entities(duplicated), 47 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  17%|█████████████████▋                                                                                    | 8/46 [00:27<00:52,  1.38s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:51,242 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:51,462 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠏ Processed 9 chunks, 190 entities(duplicated), 61 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  20%|███████████████████▉                                                                                  | 9/46 [00:28<00:48,  1.32s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:52,500 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠋ Processed 10 chunks, 220 entities(duplicated), 68 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  22%|█████████████████████▉                                                                               | 10/46 [00:29<00:44,  1.24s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:53,034 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠙ Processed 11 chunks, 248 entities(duplicated), 76 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  24%|████████████████████████▏                                                                            | 11/46 [00:30<00:36,  1.04s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:53,057 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:21:54,463 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠸ Processed 13 chunks, 292 entities(duplicated), 101 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  28%|████████████████████████████▌                                                                        | 13/46 [00:31<00:29,  1.11chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:55,590 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠼ Processed 14 chunks, 326 entities(duplicated), 107 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  30%|██████████████████████████████▋                                                                      | 14/46 [00:33<00:30,  1.05chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:21:56,337 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:01,015 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:02,055 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:02,278 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠴ Processed 15 chunks, 348 entities(duplicated), 112 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  33%|████████████████████████████████▉                                                                    | 15/46 [00:39<01:15,  2.42s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:03,845 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:04,893 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠦ Processed 16 chunks, 370 entities(duplicated), 118 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  35%|███████████████████████████████████▏                                                                 | 16/46 [00:42<01:14,  2.47s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:05,825 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:05,888 - openai._base_client - INFO - Retrying request to /chat/completions in 0.469987 seconds\n",
      "2024-12-17 22:22:06,672 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:06,808 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:07,741 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:07,961 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:09,208 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:10,030 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:11,257 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:12,587 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠧ Processed 17 chunks, 394 entities(duplicated), 124 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  37%|█████████████████████████████████████▎                                                               | 17/46 [00:50<01:53,  3.92s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:12,667 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:15,658 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠇ Processed 18 chunks, 410 entities(duplicated), 136 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  39%|███████████████████████████████████████▌                                                             | 18/46 [00:53<01:42,  3.68s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:15,901 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠏ Processed 19 chunks, 428 entities(duplicated), 150 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  41%|█████████████████████████████████████████▋                                                           | 19/46 [00:53<01:12,  2.69s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:16,102 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:18,223 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠋ Processed 20 chunks, 445 entities(duplicated), 162 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  43%|███████████████████████████████████████████▉                                                         | 20/46 [00:55<01:07,  2.58s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:19,361 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:19,492 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠙ Processed 21 chunks, 468 entities(duplicated), 168 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  46%|██████████████████████████████████████████████                                                       | 21/46 [00:56<00:54,  2.19s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:20,260 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠹ Processed 22 chunks, 490 entities(duplicated), 174 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  48%|████████████████████████████████████████████████▎                                                    | 22/46 [00:57<00:42,  1.77s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:20,868 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠸ Processed 23 chunks, 514 entities(duplicated), 185 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  50%|██████████████████████████████████████████████████▌                                                  | 23/46 [00:58<00:32,  1.43s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:21,142 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠼ Processed 24 chunks, 531 entities(duplicated), 196 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  52%|████████████████████████████████████████████████████▋                                                | 24/46 [00:58<00:23,  1.08s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:21,517 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:21,624 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠴ Processed 25 chunks, 550 entities(duplicated), 211 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  54%|██████████████████████████████████████████████████████▉                                              | 25/46 [00:59<00:19,  1.11chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:21,859 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠦ Processed 26 chunks, 579 entities(duplicated), 222 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  57%|█████████████████████████████████████████████████████████                                            | 26/46 [00:59<00:14,  1.42chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:27,440 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠧ Processed 27 chunks, 602 entities(duplicated), 244 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  59%|███████████████████████████████████████████████████████████▎                                         | 27/46 [01:04<00:41,  2.16s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:29,177 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:31,430 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:31,779 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:31,878 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:32,454 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠇ Processed 28 chunks, 621 entities(duplicated), 247 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  61%|█████████████████████████████████████████████████████████████▍                                       | 28/46 [01:09<00:54,  3.02s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:34,606 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠏ Processed 29 chunks, 645 entities(duplicated), 252 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  63%|███████████████████████████████████████████████████████████████▋                                     | 29/46 [01:12<00:46,  2.76s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:36,038 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:36,236 - openai._base_client - INFO - Retrying request to /chat/completions in 0.488199 seconds\n",
      "2024-12-17 22:22:36,343 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠋ Processed 30 chunks, 678 entities(duplicated), 269 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  65%|█████████████████████████████████████████████████████████████████▊                                   | 30/46 [01:13<00:39,  2.45s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:36,933 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:37,888 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:37,944 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:38,356 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠙ Processed 31 chunks, 721 entities(duplicated), 278 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  67%|████████████████████████████████████████████████████████████████████                                 | 31/46 [01:15<00:34,  2.32s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:39,420 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠹ Processed 32 chunks, 763 entities(duplicated), 285 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  70%|██████████████████████████████████████████████████████████████████████▎                              | 32/46 [01:16<00:27,  1.94s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:40,630 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠸ Processed 33 chunks, 784 entities(duplicated), 291 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  72%|████████████████████████████████████████████████████████████████████████▍                            | 33/46 [01:18<00:22,  1.72s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:43,002 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:45,972 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠼ Processed 34 chunks, 809 entities(duplicated), 300 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  74%|██████████████████████████████████████████████████████████████████████████▋                          | 34/46 [01:23<00:33,  2.81s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:46,004 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:46,471 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠴ Processed 35 chunks, 832 entities(duplicated), 305 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  76%|████████████████████████████████████████████████████████████████████████████▊                        | 35/46 [01:23<00:23,  2.12s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:46,999 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:47,608 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:48,329 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠦ Processed 36 chunks, 850 entities(duplicated), 322 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  78%|███████████████████████████████████████████████████████████████████████████████                      | 36/46 [01:25<00:20,  2.04s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:49,494 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:22:50,553 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠧ Processed 37 chunks, 876 entities(duplicated), 335 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  80%|█████████████████████████████████████████████████████████████████████████████████▏                   | 37/46 [01:27<00:18,  2.09s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:52,833 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠇ Processed 38 chunks, 895 entities(duplicated), 351 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  83%|███████████████████████████████████████████████████████████████████████████████████▍                 | 38/46 [01:30<00:17,  2.15s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:52,852 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠏ Processed 39 chunks, 917 entities(duplicated), 355 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  85%|█████████████████████████████████████████████████████████████████████████████████████▋               | 39/46 [01:30<00:10,  1.54s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:54,576 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠋ Processed 40 chunks, 942 entities(duplicated), 363 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  87%|███████████████████████████████████████████████████████████████████████████████████████▊             | 40/46 [01:31<00:09,  1.57s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:22:57,504 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠙ Processed 41 chunks, 966 entities(duplicated), 370 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  89%|██████████████████████████████████████████████████████████████████████████████████████████           | 41/46 [01:34<00:09,  1.98s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:23:00,411 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠹ Processed 42 chunks, 982 entities(duplicated), 384 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  91%|████████████████████████████████████████████████████████████████████████████████████████████▏        | 42/46 [01:37<00:09,  2.25s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:23:04,304 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠸ Processed 43 chunks, 1014 entities(duplicated), 389 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▍      | 43/46 [01:41<00:08,  2.75s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:23:06,248 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠼ Processed 44 chunks, 1039 entities(duplicated), 396 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  96%|████████████████████████████████████████████████████████████████████████████████████████████████▌    | 44/46 [01:43<00:05,  2.51s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:23:09,627 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠴ Processed 45 chunks, 1092 entities(duplicated), 412 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aracting entities from chunks:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████▊  | 45/46 [01:47<00:02,  2.77s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:09,691 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "⠦ Processed 46 chunks, 1342 entities(duplicated), 417 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting entities from chunks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [02:47<00:00,  3.64s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:09,861 - lightrag - INFO - Inserting entities into storage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserting entities:   0%|                                                                                                               | 1/938 [00:14<3:49:23, 14.69s/entity]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:28,475 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting entities: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 937/938 [00:18<00:00, 65.82entity/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:29,908 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting entities: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:20<00:00, 46.67entity/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:29,964 - lightrag - INFO - Inserting relationships into storage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserting relationships: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 368/368 [00:13<00:00, 27.27relationship/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:43,463 - lightrag - INFO - Inserting 938 vectors to entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                        | 0/30 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:44,858 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:44,872 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:44,946 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:44,967 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:44,970 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:44,974 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:45,028 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:45,080 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:45,128 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:45,142 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:45,157 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:45,166 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:45,172 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:45,380 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   3%|███▋                                                                                                            | 1/30 [00:02<00:59,  2.04s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:45,564 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:45,568 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  50%|███████████████████████████████████████████████████████▌                                                       | 15/30 [00:04<00:01,  8.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:48,852 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:48,861 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:48,889 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:48,951 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:48,983 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:48,990 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:49,022 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:49,032 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:49,114 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  57%|██████████████████████████████████████████████████████████████▉                                                | 17/30 [00:05<00:04,  2.94batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:49,340 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  60%|██████████████████████████████████████████████████████████████████▌                                            | 18/30 [00:05<00:03,  3.44batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:49,443 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  70%|█████████████████████████████████████████████████████████████████████████████▋                                 | 21/30 [00:06<00:01,  5.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:49,831 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  77%|█████████████████████████████████████████████████████████████████████████████████████                          | 23/30 [00:06<00:01,  6.42batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:50,028 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  93%|███████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 28/30 [00:06<00:00,  8.49batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:51,049 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:08<00:00,  3.26batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:51,801 - lightrag - INFO - Inserting 368 vectors to relationships\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "enerating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:08<00:00,  3.57batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:52,089 - openai._base_client - INFO - Retrying request to /embeddings in 0.492858 seconds\n",
      "2024-12-17 22:24:52,707 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:52,792 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:52,854 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:53,002 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:53,042 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:53,056 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:53,092 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:53,108 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:53,122 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:24:53,205 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[Aerating embeddings:  25%|████████████████████████████                                                                                    | 3/12 [00:01<00:03,  2.35batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:53,552 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[Aerating embeddings:  67%|██████████████████████████████████████████████████████████████████████████▋                                     | 8/12 [00:02<00:00,  6.87batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:24:54,041 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[Aerating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  7.79batch/s]"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "# Use the nest_asyncio package to allow running nested event loops in Jupyter Notebook without conflicts.\n",
    "nest_asyncio.apply()\n",
    "\n",
    "working_dir = \"../../applications/lightrag/data\"\n",
    "\n",
    "custom_rag = CustomLightRAG(\n",
    "    working_dir=working_dir,\n",
    "    graph_storage=\"TigerGraphStorage\",\n",
    ")\n",
    "\n",
    "with open(working_dir + \"/input/fin.txt\") as f:\n",
    "    custom_rag.insert(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a68c67-9e55-4b57-82b1-c66a9c674f12",
   "metadata": {},
   "source": [
    "## Querying\n",
    "The following code demonstrates how to perform a query in LightRAG using the TigerGraph graph storage implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "917a843d-40f1-4843-83d4-1eac4eb6869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:27:48,142 - lightrag - INFO - Logger initialized for working directory: ../../applications/lightrag/data\n",
      "2024-12-17 22:27:48,145 - lightrag - INFO - Load KV llm_response_cache with 0 data\n",
      "2024-12-17 22:27:48,148 - lightrag - INFO - Load KV full_docs with 1 data\n",
      "2024-12-17 22:27:48,153 - lightrag - INFO - Load KV text_chunks with 46 data\n",
      "2024-12-17 22:27:48,155 - lightrag - INFO - TigerGraph connection configuration retrieved successfully.\n",
      "2024-12-17 22:27:48,248 - lightrag - INFO - Undirected graph initialized successfully with graph_name 'LightRAG'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [02:56<00:00, 14.71s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:27:48,328 - nano-vectordb - INFO - Load (938, 1536) data\n",
      "2024-12-17 22:27:48,332 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_entities.json'} 938 data\n",
      "2024-12-17 22:27:48,376 - nano-vectordb - INFO - Load (368, 1536) data\n",
      "2024-12-17 22:27:48,379 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_relationships.json'} 368 data\n",
      "2024-12-17 22:27:48,382 - nano-vectordb - INFO - Load (46, 1536) data\n",
      "2024-12-17 22:27:48,382 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_chunks.json'} 46 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 22:27:49,499 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:27:49,500 - lightrag - INFO - kw_prompt result:\n",
      "{\n",
      "  \"high_level_keywords\": [\"Financial health\", \"Company performance\", \"Economic assessment\"],\n",
      "  \"low_level_keywords\": [\"Revenue\", \"Expenses\", \"Profit margins\", \"Assets\", \"Liabilities\"]\n",
      "}\n",
      "2024-12-17 22:27:50,009 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:27:52,889 - lightrag - INFO - Local query uses 60 entites, 35 relations, 3 text units\n",
      "2024-12-17 22:27:53,582 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-17 22:27:54,039 - lightrag - WARNING - Some edges are missing, maybe the storage is damaged\n",
      "2024-12-17 22:27:55,274 - lightrag - INFO - Global query uses 48 entites, 58 relations, 3 text units\n",
      "2024-12-17 22:28:08,736 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "------------------- Query Result:  -------------------\n",
      "## Financial Overview of CytoSorbents Corporation\n",
      "\n",
      "CytoSorbents Corporation is engaged in the development and commercialization of medical technologies, particularly focusing on its flagship product, CytoSorb, which is a blood purification device. The company's financial health can be assessed through various aspects, including revenue generation, funding activities, and market strategies.\n",
      "\n",
      "### Funding and Capital Raising\n",
      "\n",
      "The company has actively pursued multiple avenues of funding to support its operations and research initiatives. Notably, in July 2020, CytoSorbents closed an underwritten public offering that raised approximately $57.5 million in gross proceeds. After accounting for underwriting fees and other expenses, the net proceeds amounted to about $53.8 million. This influx of capital has been crucial in enabling the company to fund research and development efforts, expand manufacturing capabilities, and enhance its product offerings.\n",
      "\n",
      "The corporation has also secured significant non-dilutive funding from various governmental organizations, amounting to approximately $28.4 million since 2012. This includes grants and contracts from notable entities such as DARPA, the U.S. Army, and the National Heart, Lung, and Blood Institute. These diverse funding sources underscore the company's commitment to innovation and its capacity to attract financial support for its projects.\n",
      "\n",
      "### Revenue Generation and Market Presence\n",
      "\n",
      "CytoSorbents’ presence in the European market, especially in Germany, reflects a growing demand for its products. The company has achieved dedicated reimbursement codes in Germany since January 2017, facilitating better reimbursement for its CytoSorb device in medical procedures. The recognition and acceptance of CytoSorb in over 40,000 treatments highlight its integration into critical care practices within Europe.\n",
      "\n",
      "In addition, the company's attempts to penetrate the U.S. market are vital for its long-term growth. While CytoSorb is not yet approved for widespread use in the U.S., it received FDA Emergency Use Authorization in April 2020 for COVID-19 treatment applications. This regulatory achievement has opened potential revenue avenues from a significant market, provided that further approvals for various clinical applications can be secured.\n",
      "\n",
      "### Cost Management and Financial Risks\n",
      "\n",
      "Despite the positive outlook from funding and revenue generation, the company faces financial pressures stemming from operational challenges, particularly in terms of compliance with regulatory frameworks. The potential impact of the American Taxpayer Relief Act may create uncertainty regarding government funding, which poses a risk to the company's financial stability. Furthermore, the company's operations may be affected by sequestration and potential automatic cuts in federal spending.\n",
      "\n",
      "Royalty obligations stemming from agreements involving the use of patented technology also impose additional costs, with royalty payments amounting to approximately $1.17 million for the year ended December 31, 2020. Such obligations are indicative of the financial commitments related to intellectual property and product development.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Overall, CytoSorbents Corporation demonstrates a robust financial position characterized by diverse funding avenues, a solid presence in international markets, and efforts to penetrate the U.S. healthcare landscape. However, it faces challenges related to regulatory compliance and the financial implications of obligations arising from its operational frameworks. The company's ability to navigate these challenges while leveraging its technological advancements will be crucial for sustaining its growth trajectory and enhancing financial health moving forward.\n"
     ]
    }
   ],
   "source": [
    "from lightrag import QueryParam\n",
    "\n",
    "custom_rag = CustomLightRAG(\n",
    "    working_dir=working_dir,\n",
    "    graph_storage=\"TigerGraphStorage\",\n",
    ")\n",
    "\n",
    "query = \"What is the overall financial health of the company?\"\n",
    "\n",
    "result = custom_rag.query(query=query, param=QueryParam(mode=\"hybrid\"))\n",
    "\n",
    "print(\"------------------- Query Result:  -------------------\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
