{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TigerGraphX","text":"<p>TigerGraphX is a high-level Python library offering a unified, Python-native interface for graph databases, advanced analytics, and GraphRAG workflows. Combining the simplicity of NetworkX with the advanced capabilities of TigerGraph, including tgCloud, it empowers Python developers to harness the power of graphs without the need to learn query languages like Cypher or GSQL.</p> <p>TigerGraphX is designed for two main audiences, catering to different use cases while maintaining an easy-to-use, developer-friendly experience:</p>"},{"location":"#who-should-use-tigergraphx","title":"Who Should Use TigerGraphX?","text":""},{"location":"#1-python-developers-for-graph-analytics-with-tigergraph","title":"1. Python Developers for Graph Analytics with TigerGraph","text":"<p>If you\u2019re a Python developer interested in performing graph analytics with TigerGraph, TigerGraphX provides:</p> <ul> <li>Python-Native APIs: No need to learn complex query languages like GSQL or Cypher.</li> <li>Seamless Integration: Easily perform CRUD operations, multi-hop queries, and advanced analytics directly from Python.</li> <li>Scalability: Leverage the powerful performance of TigerGraph for large-scale graph processing.</li> </ul> <p>Start here:</p> <ul> <li>Introduction to TigerGraphX: Discover the features and benefits of using TigerGraphX for graph database management and analytics.</li> <li>Getting Started: Learn how to install and set up TigerGraphX.</li> <li>Quick Start Guide: Quickly set up TigerGraphX and build your first graph with this step-by-step guide.</li> </ul>"},{"location":"#2-python-developers-for-graphrag-workflows","title":"2. Python Developers for GraphRAG Workflows","text":"<p>If you\u2019re a Python developer building GraphRAG applications, TigerGraphX enables you to:</p> <ul> <li>Use TigerGraph as a scalable database for storing and retrieving graph and vector data generated by GraphRAG algorithms.</li> <li>Build token-aware LLM workflows for advanced AI applications.</li> <li>Utilize context builders to streamline data preparation for large language models.</li> </ul> <p>Start here:</p> <ul> <li>Getting Started: Learn how to install and set up TigerGraphX.</li> <li>GraphRAG Overview: Explore how TigerGraphX integrates with GraphRAG workflows.</li> <li>Supporting Microsoft\u2019s GraphRAG: An example of using TigerGraphX for GraphRAG workflows.</li> </ul> <p>Start unlocking the power of graphs with TigerGraphX today!</p>"},{"location":"introduction/","title":"TigerGraphX: Unified Graph Solutions for Python Developers","text":""},{"location":"introduction/#what-is-tigergraphx","title":"What is TigerGraphX?","text":"<p>TigerGraphX is a high-level Python library offering a unified, Python-native interface for graph databases, advanced analytics, and GraphRAG workflows. Combining the simplicity of NetworkX with the advanced capabilities of TigerGraph, including tgCloud, it empowers Python developers to harness the power of graphs without the need to learn query languages like Cypher or GSQL.</p>"},{"location":"introduction/#core-mission","title":"Core Mission","text":"<p>TigerGraphX seeks to democratize graph technology by providing an intuitive, all-encompassing framework that integrates and provides direct connection to:</p> <ul> <li>TigerGraph Database capabilities</li> <li>TigerGraph Vector Database functionality</li> <li>Large Language Model (LLM) integration</li> <li>TigerGraph\u2019s GraphRAG support for intelligent workflow</li> </ul>"},{"location":"introduction/#key-features","title":"Key Features","text":""},{"location":"introduction/#1-schema-management","title":"1. Schema Management","text":"<ul> <li>Easily create and modify schemas using YAML, JSON, or Python dictionaries.</li> <li>No GSQL knowledge is required.</li> <li>Pythonic tools for designing database structures effortlessly.</li> </ul>"},{"location":"introduction/#2-data-loading","title":"2. Data Loading","text":"<ul> <li>Automated loading jobs for streamlined data imports.</li> <li>High-efficiency workflows with support for Parquet files.</li> <li>Simplified data ingestion processes for faster setup.</li> </ul>"},{"location":"introduction/#3-graph-library-interface","title":"3. Graph Library Interface","text":"<ul> <li>Python-native APIs for CRUD operations.</li> <li>Comprehensive tools for graph reporting and visualization.</li> <li>Built-in graph algorithms including centrality, community detection, and path analysis algorithms</li> </ul>"},{"location":"introduction/#4-graph-query-interface","title":"4. Graph Query Interface","text":"<ul> <li>Simplified advanced querying with intuitive APIs.</li> <li>Seamless integration into analytics workflows via DataFrame outputs.</li> <li>Support for advanced multi-hop query traversal and manipulation</li> </ul>"},{"location":"introduction/#5-vector-search-capabilities","title":"5. Vector Search Capabilities","text":"<ul> <li>AI-driven applications with integrated vector embeddings.</li> <li>Efficient top-K entity retrieval for enhanced intelligence.</li> <li>Ideal for recommendation systems and contextual analysis.</li> </ul>"},{"location":"introduction/#6-llm-integration-and-graphrag-support","title":"6. LLM Integration and GraphRAG support","text":"<ul> <li>Full support for GraphRAG workflows.</li> <li>Flexible, token-aware context builders for advanced applications.</li> <li>Tools for token optimization and seamless LLM integration.</li> </ul>"},{"location":"introduction/#7-machine-learning-ready-planned-feature","title":"7. Machine Learning Ready [Planned Feature]","text":"<ul> <li>Seamless integration with popular ML libraries</li> <li>Graph feature extraction</li> <li>Native support for graph neural networks (GNNs)</li> </ul>"},{"location":"introduction/#why-choose-tigergraphx","title":"Why Choose TigerGraphX?","text":"<p>TigerGraphX redefines graph technology by making advanced and powerful graph operations accessible and intuitive for Python developers. With its unified, user-friendly interface, TigerGraphX bridges the gap between simplicity and scalability, enabling developers to:</p> <ul> <li>Leverage TigerGraph\u2019s unmatched scalability for high-performance graph processing.  </li> <li>Enjoy the familiarity of tools like NetworkX while unlocking enterprise-grade graph capabilities.  </li> <li>Access advanced graph analytics with ease, reducing the learning curve and technical barriers.  </li> <li>Develop intelligent, context-aware GraphRAG applications effortlessly with token-aware workflows and streamlined context builders.</li> </ul> <p>TigerGraphX empowers developers to explore, analyze, and build with graphs like never before\u2014efficiently and effectively.</p>"},{"location":"introduction/#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started: Learn how to install and set up TigerGraphX.</li> <li>GraphRAG Overview: Explore how TigerGraphX integrates with GraphRAG workflows.</li> </ul> <p>Start unlocking the power of graphs with TigerGraphX today!</p>"},{"location":"getting_started/installation/","title":"Installation Guide","text":"<p>Follow this guide to install and set up TigerGraphX in your environment.</p>"},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<p>This project requires Python 3.12 and TigerGraph. Ensure you meet the following prerequisites before proceeding:</p>"},{"location":"getting_started/installation/#1-python-312","title":"1. Python 3.12","text":"<ul> <li>Please ensure Python 3.12 is installed on your system.</li> <li>You can download and install it from the official Python website.</li> </ul>"},{"location":"getting_started/installation/#2-tigergraph","title":"2. TigerGraph","text":"<p>TigerGraph is required for this project and can be set up in one of the following ways:</p> <ul> <li>TigerGraph DB: Install and configure a local instance of TigerGraph.</li> <li>TigerGraph Cloud: Use a cloud-hosted instance of TigerGraph.</li> </ul> <p>It is recommended to use TigerGraph LTS (Long-Term Support) Versions, which can be downloaded from the TigerGraph Downloads page. To enable support for TigerVector and leverage advanced features like hybrid retrieval, ensure you are using TigerGraph 4.2 or above.</p> <p>Refer to the official TigerGraph Documentation for detailed installation and configuration instructions.</p>"},{"location":"getting_started/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"getting_started/installation/#option-1-install-from-pypi","title":"Option 1: Install from PyPI","text":"<p>The simplest way to get started with TigerGraphX is by installing it directly from PyPI. Using a virtual environment is recommended to ensure a clean and isolated setup.</p> <p>To install TigerGraphX, run: <pre><code>pip install tigergraphx\n</code></pre></p> <p>This allows you to quickly start using the library without needing the source code.</p>"},{"location":"getting_started/installation/#verify-installation","title":"Verify Installation","text":"<p>After installing, verify that TigerGraphX is installed correctly by running: <pre><code>python -c \"import tigergraphx; print('TigerGraphX installed successfully!')\"\n</code></pre></p> <p>If the installation was successful, you will see: <pre><code>TigerGraphX installed successfully!\n</code></pre></p> <p>This ensures that the library is properly installed and ready for use.</p>"},{"location":"getting_started/installation/#option-2-build-from-source-code","title":"Option 2: Build from Source Code","text":"<p>If you want to modify or explore the source code, you can install TigerGraphX from its GitHub repository. The source code is available here: TigerGraphX on GitHub.</p> <p>This project uses Poetry to manage dependencies. If you don\u2019t have Poetry installed, follow the instructions on the Poetry website.</p> <p>Once Poetry is installed, clone the repository, navigate to the project\u2019s root directory, and use one of the following commands to install dependencies based on your needs:</p>"},{"location":"getting_started/installation/#core-installation","title":"Core Installation","text":"<p>If you need only the core functionality of TigerGraphX (without running application examples like GraphRAG, unit tests, or integration tests), run: <pre><code>poetry install --without dev\n</code></pre></p> <p>This command will:</p> <ul> <li>Install only the dependencies required for the core features of TigerGraphX.</li> </ul>"},{"location":"getting_started/installation/#development-installation","title":"Development Installation","text":"<p>If you\u2019re contributing to the project or want to use advanced features like running the GraphRAG examples or test cases, run: <pre><code>poetry install --with dev\n</code></pre></p> <p>This command will:</p> <ul> <li>Install all core dependencies.</li> <li>Include development dependencies defined under <code>[tool.poetry.group.dev.dependencies]</code> in <code>pyproject.toml</code>.</li> </ul>"},{"location":"getting_started/installation/#verify-your-installation","title":"Verify Your Installation","text":"<p>After installing dependencies, verify your setup by listing the installed packages: <pre><code>poetry show --with dev\n</code></pre></p> <p>This ensures all required dependencies (including optional ones) are successfully installed.</p>"},{"location":"getting_started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start: Learn how to build your first graph with TigerGraphX.</li> </ul> <p>Start unlocking the power of graphs with TigerGraphX today!</p>"},{"location":"getting_started/quick_start/","title":"Quick Start Guide","text":"In\u00a0[3]: Copied! <pre>from tigergraphx import Graph, GraphSchema, TigerGraphConnectionConfig\ngraph_schema = GraphSchema.ensure_config({\n    \"graph_name\": \"Social\",\n    \"nodes\": {\n        \"Person\": {\n            \"primary_key\": \"name\",\n            \"attributes\": {\n                \"name\": \"STRING\",\n                \"age\": \"UINT\",\n            },\n        },\n    },\n    \"edges\": {\n        \"Friendship\": {\n            \"is_directed_edge\": False,\n            \"from_node_type\": \"Person\",\n            \"to_node_type\": \"Person\",\n            \"attributes\": {\n                \"closeness\": \"DOUBLE\",\n            },\n        },\n    },\n})\n</pre> from tigergraphx import Graph, GraphSchema, TigerGraphConnectionConfig graph_schema = GraphSchema.ensure_config({     \"graph_name\": \"Social\",     \"nodes\": {         \"Person\": {             \"primary_key\": \"name\",             \"attributes\": {                 \"name\": \"STRING\",                 \"age\": \"UINT\",             },         },     },     \"edges\": {         \"Friendship\": {             \"is_directed_edge\": False,             \"from_node_type\": \"Person\",             \"to_node_type\": \"Person\",             \"attributes\": {                 \"closeness\": \"DOUBLE\",             },         },     }, }) In\u00a0[4]: Copied! <pre>connection = TigerGraphConnectionConfig.ensure_config({\n    \"host\": \"http://127.0.0.1\",\n    \"user_name\": \"tigergraph\",\n    \"password\": \"tigergraph\",\n})\n</pre> connection = TigerGraphConnectionConfig.ensure_config({     \"host\": \"http://127.0.0.1\",     \"user_name\": \"tigergraph\",     \"password\": \"tigergraph\", }) In\u00a0[5]: Copied! <pre>G = Graph(graph_schema, connection)\n</pre> G = Graph(graph_schema, connection) In\u00a0[6]: Copied! <pre>G.add_node(\"Alice\", \"Person\", age=25)\nG.add_node(\"Michael\", \"Person\", age=28)\nG.add_edge(\"Alice\", \"Michael\", closeness=0.98)\n</pre> G.add_node(\"Alice\", \"Person\", age=25) G.add_node(\"Michael\", \"Person\", age=28) G.add_edge(\"Alice\", \"Michael\", closeness=0.98) In\u00a0[7]: Copied! <pre>print(G.has_node(\"Alice\"))\n</pre> print(G.has_node(\"Alice\")) <pre>True\n</pre> In\u00a0[8]: Copied! <pre>print(G.has_node(\"Michael\"))\n</pre> print(G.has_node(\"Michael\")) <pre>True\n</pre> <p>Since the 'Friendship' edge is undirected, both 'Alice -&gt; Michael' and 'Michael -&gt; Alice' are valid and accessible.</p> In\u00a0[9]: Copied! <pre>print(G.has_edge(\"Alice\", \"Michael\"))\n</pre> print(G.has_edge(\"Alice\", \"Michael\")) <pre>True\n</pre> In\u00a0[10]: Copied! <pre>print(G.has_edge(\"Michael\", \"Alice\"))\n</pre> print(G.has_edge(\"Michael\", \"Alice\")) <pre>True\n</pre> In\u00a0[11]: Copied! <pre>print(G.get_node_data(\"Alice\"))\n</pre> print(G.get_node_data(\"Alice\")) <pre>{'name': 'Alice', 'age': 25}\n</pre> In\u00a0[12]: Copied! <pre>print(G.get_edge_data(\"Alice\", \"Michael\"))\n</pre> print(G.get_edge_data(\"Alice\", \"Michael\")) <pre>{'closeness': 0.98}\n</pre> In\u00a0[13]: Copied! <pre>print(G.nodes[\"Alice\"])\n</pre> print(G.nodes[\"Alice\"]) <pre>{'name': 'Alice', 'age': 25}\n</pre> In\u00a0[14]: Copied! <pre>print(G.nodes[\"Michael\"][\"age\"])\n</pre> print(G.nodes[\"Michael\"][\"age\"]) <pre>28\n</pre> <p>Note: The Edge View feature is planned for future releases.</p> In\u00a0[15]: Copied! <pre>print(G.degree(\"Alice\"))\n</pre> print(G.degree(\"Alice\")) <pre>1\n</pre> In\u00a0[16]: Copied! <pre>neighbors = G.get_neighbors(\"Alice\")\nprint(neighbors)\n</pre> neighbors = G.get_neighbors(\"Alice\") print(neighbors) <pre>      name  age\n0  Michael   28\n</pre> In\u00a0[17]: Copied! <pre>print(type(neighbors))\n</pre> print(type(neighbors)) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</pre> In\u00a0[18]: Copied! <pre>print(G.number_of_nodes())\n</pre> print(G.number_of_nodes()) <pre>2\n</pre> In\u00a0[19]: Copied! <pre>print(G.number_of_edges())\n</pre> print(G.number_of_edges()) <pre>1\n</pre>"},{"location":"getting_started/quick_start/#quick-start-guide","title":"Quick Start Guide\u00b6","text":"<p>Follow this guide to quickly set up TigerGraphX and build your first graph. This guide assumes that you have already installed TigerGraphX and its dependencies as described in the Installation Guide.</p> <p>To run this Jupyter Notebook, you can download the original <code>.ipynb</code> file from quick_start.ipynb.</p>"},{"location":"getting_started/quick_start/#create-a-graph","title":"Create a Graph\u00b6","text":""},{"location":"getting_started/quick_start/#define-a-graph-schema","title":"Define a Graph Schema\u00b6","text":"<p>TigerGraph is a schema-based database, which requires defining a schema to structure your graph. This schema specifies the graph name, nodes (vertices), edges (relationships), and their respective attributes.</p> <p>In this example, we will create a graph named \"Social\" that includes one node type, \"Person,\" and one directed edge type, \"Friendship.\" Note that you must define the primary key for each node type, indicate whether an edge type is directed or undirected, and specify the source and target node types for each edge type.</p>"},{"location":"getting_started/quick_start/#define-the-tigergraph-connection-configuration","title":"Define the TigerGraph Connection Configuration\u00b6","text":"<p>In addition to defining the schema, a connection configuration is necessary to establish communication with the TigerGraph server.</p>"},{"location":"getting_started/quick_start/#create-a-graph","title":"Create a Graph\u00b6","text":"<p>Running the following command will create a graph using the user-defined schema if it does not already exist. If the graph exists, the command will return the existing graph. To overwrite the existing graph, set the drop_existing_graph parameter to True. Note that creating the graph may take several seconds.</p>"},{"location":"getting_started/quick_start/#nodes-and-edges","title":"Nodes and Edges\u00b6","text":""},{"location":"getting_started/quick_start/#add-nodes-and-edges","title":"Add Nodes and Edges\u00b6","text":"<p>Note: This example demonstrates how to easily add nodes and edges using the API. However, adding nodes and edges individually may not be efficient for large-scale operations. For better performance when loading data into TigerGraph, it is recommended to use a loading job. Nonetheless, these examples are ideal for quickly getting started.</p>"},{"location":"getting_started/quick_start/#check-if-nodes-and-edges-exist","title":"Check if Nodes and Edges Exist\u00b6","text":""},{"location":"getting_started/quick_start/#display-node-and-edge-attributes","title":"Display Node and Edge Attributes\u00b6","text":""},{"location":"getting_started/quick_start/#using-get_node_data-and-get_edge_data-functions","title":"Using <code>get_node_data</code> and <code>get_edge_data</code> Functions\u00b6","text":""},{"location":"getting_started/quick_start/#using-node-view","title":"Using Node View\u00b6","text":""},{"location":"getting_started/quick_start/#display-the-degree-of-nodes","title":"Display the Degree of Nodes\u00b6","text":""},{"location":"getting_started/quick_start/#retrieve-the-neighbors-of-a-node","title":"Retrieve the Neighbors of a Node\u00b6","text":""},{"location":"getting_started/quick_start/#graph-statistics","title":"Graph Statistics\u00b6","text":""},{"location":"getting_started/quick_start/#display-the-number-of-nodes","title":"Display the Number of Nodes\u00b6","text":""},{"location":"getting_started/quick_start/#display-the-number-of-edges","title":"Display the Number of Edges\u00b6","text":""},{"location":"getting_started/quick_start/#whats-next","title":"What\u2019s Next?\u00b6","text":"<p>Now that you\u2019ve set up your graph and performed basic operations, you can explore more advanced features of TigerGraphX:</p> <ul> <li>GraphRAG Overview: Learn about integrating graphs with LLMs.</li> <li>API Reference: Dive deeper into TigerGraphX APIs.</li> </ul> <p>Start unlocking the power of graphs with TigerGraphX today!</p>"},{"location":"graphrag/graphrag_overview/","title":"TigerGraph: Unlocking the Potential of GraphRAG","text":""},{"location":"graphrag/graphrag_overview/#overview","title":"Overview","text":"<p>TigerGraph is a highly scalable and efficient graph database, making it the ideal foundation for advanced GraphRAG workflows. It excels in handling both graph and vector data, enabling seamless integration and performance at scale. With built-in support for complex queries, multi-hop traversals, and real-time analytics, TigerGraph ensures fast and reliable results. Its versatility and performance make it the ideal choice for powering data-intensive workflows, while TigerGraphX simplifies access with a Python-native interface.</p> <p></p>"},{"location":"graphrag/graphrag_overview/#why-tigergraph-for-graphrag","title":"Why TigerGraph for GraphRAG?","text":""},{"location":"graphrag/graphrag_overview/#1-scalability-and-performance","title":"1. Scalability and Performance","text":"<p>TigerGraph excels in handling massive datasets with high-speed multi-hop queries and vector search capabilities. It is ideal for real-world GraphRAG applications that demand extensive and efficient data processing.</p>"},{"location":"graphrag/graphrag_overview/#2-unified-graph-and-vector-data-support","title":"2. Unified Graph and Vector Data Support","text":"<p>With native support for schema-defined nodes, edges, and vectors, TigerGraph streamlines data integration. Its advanced query optimization enables efficient graph traversal and vector-based retrieval, which is perfectly suited for LLM workflows.</p>"},{"location":"graphrag/graphrag_overview/#3-cost-effectiveness","title":"3. Cost-Effectiveness","text":"<p>TigerGraph reduces computational overhead through optimized queries and highly efficient storage, significantly cutting infrastructure costs while maintaining top-tier performance.</p>"},{"location":"graphrag/graphrag_overview/#4-flexibility-and-hybrid-integration","title":"4. Flexibility and Hybrid Integration","text":"<p>Seamlessly combines structured, semantic, and vector-based retrieval methods in one unified platform. Its compatibility with vector search and LLMs enables advanced hybrid retrieval strategies, unlocking new possibilities for GraphRAG workflows.</p>"},{"location":"graphrag/graphrag_overview/#graphrag-workflow-with-tigergraph","title":"GraphRAG Workflow with TigerGraph","text":""},{"location":"graphrag/graphrag_overview/#1-schema-design","title":"1. Schema Design","text":"<p>Define the graph schema with nodes, edges, and attributes tailored to your application, leveraging TigerGraph\u2019s native support for structured graph data.</p>"},{"location":"graphrag/graphrag_overview/#2-data-preparation-and-loading","title":"2. Data Preparation and Loading","text":"<p>Transform raw data into TigerGraph-compatible formats, including graph structures and embeddings, and load it efficiently into TigerGraph using TigerGraphX.</p>"},{"location":"graphrag/graphrag_overview/#3-knowledge-graph-management-and-analysis","title":"3. Knowledge Graph Management and Analysis","text":"<p>Maintain and enhance the knowledge graph to ensure data quality, relevance, and scalability. Perform in-depth analysis to uncover patterns, infer insights, and optimize data retrieval strategies; ensure the knowledge graph remains a dynamic, accurate, and actionable source of information, enriching context for LLMs while supporting explainability and scalability in the GraphRAG workflow.</p>"},{"location":"graphrag/graphrag_overview/#4-hybrid-retrieval","title":"4. Hybrid Retrieval","text":"<p>Combine structured queries, semantic search, and vector-based methods to fetch relevant data and embeddings from TigerGraph for context construction.</p>"},{"location":"graphrag/graphrag_overview/#5-context-building","title":"5. Context Building","text":"<p>Use TigerGraphX to process retrieved data, making it token-aware and formatted to meet the requirements of LLMs.</p>"},{"location":"graphrag/graphrag_overview/#6-llm-integration","title":"6. LLM Integration","text":"<p>Pass the context to an LLM to generate responses, enabling advanced GraphRAG workflows with seamless data flow and high efficiency.</p>"},{"location":"graphrag/graphrag_overview/#two-options-for-implementing-graphrag-with-tigergraph","title":"Two Options for Implementing GraphRAG with TigerGraph","text":"<p>There are two approaches to implementing GraphRAG with TigerGraph.</p>"},{"location":"graphrag/graphrag_overview/#1-tigergraph-as-a-storage-and-retrieval-engine","title":"1. TigerGraph as a Storage and Retrieval Engine","text":"<p>The first approach primarily utilizes TigerGraph for storing and retrieving graph/vector data. TigerGraphX provides interfaces similar to NetworkX, allowing seamless integration with GraphRAG applications. This approach is recommended for GraphRAG solutions like LightRAG and Nano-GraphRAG, which abstract their storage layers (e.g., graph storage, key-value storage, and vector storage). Here, you only need to implement these layers in a way that aligns with TigerGraph.</p>"},{"location":"graphrag/graphrag_overview/#2-tigergraph-for-storage-retrieval-and-llm-tasks","title":"2. TigerGraph for Storage, Retrieval, and LLM Tasks","text":"<p>The second approach extends beyond storage and retrieval by leveraging TigerGraphX for tasks related to large language models (LLMs), such as chat or embedding generation. This approach is suitable for complex projects like Microsoft's GraphRAG. As of December 2024, Microsoft's GraphRAG has not yet abstracted its storage layer, making it challenging to replace the indexing process. However, TigerGraphX can be used to convert the results of the indexing process (e.g., Parquet files) into a format supported by TigerGraph. These results can then be imported into TigerGraph, and TigerGraphX can handle the querying process without relying on Microsoft\u2019s GraphRAG.</p>"},{"location":"graphrag/graphrag_overview/#demonstrations","title":"Demonstrations","text":"<p>Both methods are demonstrated on the following pages, each with a real-world project:</p> <ul> <li>LightRAG: Refer to LightRAG for the first approach.</li> <li>Microsoft's GraphRAG: Refer to Microsoft GraphRAG: Part 1 for the second approach.</li> </ul> <p>Start transforming your GraphRAG workflows with the power of TigerGraphX today!</p>"},{"location":"graphrag/lightrag/","title":"Supporting LightRAG","text":"<p>LightRAG is an open-source RAG system that enhances LLMs by integrating graph-based structures into text indexing and retrieval. It overcomes the limitations of traditional RAG systems, such as fragmented answers and weak contextual awareness, by enabling dual-level retrieval for more comprehensive knowledge discovery. With support for incremental data updates, LightRAG ensures timely integration of new information while delivering improved retrieval accuracy and efficiency.</p> <p>To run this Jupyter Notebook, you can download the original <code>.ipynb</code> file from lightrag.ipynb.</p> In\u00a0[2]: Copied! <pre>import os\nfrom dataclasses import dataclass\nimport numpy as np\n\nfrom lightrag.base import BaseGraphStorage\nfrom lightrag.utils import logger\nfrom tigergraphx import UndiGraph, TigerGraphConnectionConfig\n\n\n@dataclass\nclass TigerGraphStorage(BaseGraphStorage):\n    def __post_init__(self):\n        try:\n            # Retrieve connection configuration from environment variables\n            connection_config = {\n                \"host\": os.environ[\"TG_HOST\"],\n                \"username\": os.environ[\"TG_USERNAME\"],\n                \"password\": os.environ[\"TG_PASSWORD\"],\n            }\n            logger.info(\"TigerGraph connection configuration retrieved successfully.\")\n            # Initialize the graph\n            self._graph = UndiGraph(\n                graph_name=\"LightRAG\",\n                node_type=\"MyNode\",\n                edge_type=\"MyEdge\",\n                node_primary_key=\"id\",\n                node_attributes={\n                    \"id\": \"STRING\",\n                    \"entity_type\": \"STRING\",\n                    \"description\": \"STRING\",\n                    \"source_id\": \"STRING\",\n                },\n                edge_attributes={\n                    \"weight\": \"DOUBLE\",\n                    \"description\": \"STRING\",\n                    \"keywords\": \"STRING\",\n                    \"source_id\": \"STRING\",\n                },\n                tigergraph_connection_config=TigerGraphConnectionConfig.ensure_config(\n                    connection_config\n                ),\n            )\n            logger.info(\n                \"Undirected graph initialized successfully with graph_name 'LightRAG'.\"\n            )\n        except KeyError as e:\n            logger.error(f\"Environment variable {str(e)} is missing.\")\n            raise\n        except Exception as e:\n            logger.error(f\"An error occurred during initialization: {e}\")\n            raise\n\n    @staticmethod\n    def clean_quotes(value: str) -&gt; str:\n        \"\"\"Remove leading and trailing &amp;quot; from a string if present.\"\"\"\n        if value.startswith('\"') and value.endswith('\"'):\n            return value[1:-1]\n        return value\n\n    async def has_node(self, node_id: str) -&gt; bool:\n        return self._graph.has_node(self.clean_quotes(node_id))\n\n    async def has_edge(self, source_node_id: str, target_node_id: str) -&gt; bool:\n        return self._graph.has_edge(\n            self.clean_quotes(source_node_id), self.clean_quotes(target_node_id)\n        )\n\n    async def node_degree(self, node_id: str) -&gt; int:\n        result = self._graph.degree(self.clean_quotes(node_id))\n        return result\n\n    async def edge_degree(self, src_id: str, tgt_id: str) -&gt; int:\n        return self._graph.degree(self.clean_quotes(src_id)) + self._graph.degree(\n            self.clean_quotes(tgt_id)\n        )\n\n    async def get_node(self, node_id: str) -&gt; dict | None:\n        result = self._graph.get_node_data(self.clean_quotes(node_id))\n        return result\n\n    async def get_edge(self, source_node_id: str, target_node_id: str) -&gt; dict | None:\n        result = self._graph.get_edge_data(\n            self.clean_quotes(source_node_id), self.clean_quotes(target_node_id)\n        )\n        return result\n\n    async def get_node_edges(self, source_node_id: str) -&gt; list[tuple[str, str]] | None:\n        source_node_id = self.clean_quotes(source_node_id)\n        if self._graph.has_node(source_node_id):\n            edges = self._graph.get_node_edges(source_node_id)\n            return list(edges)\n        return None\n\n    async def upsert_node(self, node_id: str, node_data: dict[str, str]):\n        node_id = self.clean_quotes(node_id)\n        self._graph.add_node(node_id, **node_data)\n\n    async def upsert_edge(\n        self, source_node_id: str, target_node_id: str, edge_data: dict[str, str]\n    ):\n        source_node_id = self.clean_quotes(source_node_id)\n        target_node_id = self.clean_quotes(target_node_id)\n        self._graph.add_edge(source_node_id, target_node_id, **edge_data)\n\n    async def delete_node(self, node_id: str):\n        if self._graph.has_node(node_id):\n            self._graph.remove_node(node_id)\n            logger.info(f\"Node {node_id} deleted from the graph.\")\n        else:\n            logger.warning(f\"Node {node_id} not found in the graph for deletion.\")\n\n    async def embed_nodes(self, algorithm: str) -&gt; tuple[np.ndarray, list[str]]:\n        return np.array([]), []\n</pre> import os from dataclasses import dataclass import numpy as np  from lightrag.base import BaseGraphStorage from lightrag.utils import logger from tigergraphx import UndiGraph, TigerGraphConnectionConfig   @dataclass class TigerGraphStorage(BaseGraphStorage):     def __post_init__(self):         try:             # Retrieve connection configuration from environment variables             connection_config = {                 \"host\": os.environ[\"TG_HOST\"],                 \"username\": os.environ[\"TG_USERNAME\"],                 \"password\": os.environ[\"TG_PASSWORD\"],             }             logger.info(\"TigerGraph connection configuration retrieved successfully.\")             # Initialize the graph             self._graph = UndiGraph(                 graph_name=\"LightRAG\",                 node_type=\"MyNode\",                 edge_type=\"MyEdge\",                 node_primary_key=\"id\",                 node_attributes={                     \"id\": \"STRING\",                     \"entity_type\": \"STRING\",                     \"description\": \"STRING\",                     \"source_id\": \"STRING\",                 },                 edge_attributes={                     \"weight\": \"DOUBLE\",                     \"description\": \"STRING\",                     \"keywords\": \"STRING\",                     \"source_id\": \"STRING\",                 },                 tigergraph_connection_config=TigerGraphConnectionConfig.ensure_config(                     connection_config                 ),             )             logger.info(                 \"Undirected graph initialized successfully with graph_name 'LightRAG'.\"             )         except KeyError as e:             logger.error(f\"Environment variable {str(e)} is missing.\")             raise         except Exception as e:             logger.error(f\"An error occurred during initialization: {e}\")             raise      @staticmethod     def clean_quotes(value: str) -&gt; str:         \"\"\"Remove leading and trailing \" from a string if present.\"\"\"         if value.startswith('\"') and value.endswith('\"'):             return value[1:-1]         return value      async def has_node(self, node_id: str) -&gt; bool:         return self._graph.has_node(self.clean_quotes(node_id))      async def has_edge(self, source_node_id: str, target_node_id: str) -&gt; bool:         return self._graph.has_edge(             self.clean_quotes(source_node_id), self.clean_quotes(target_node_id)         )      async def node_degree(self, node_id: str) -&gt; int:         result = self._graph.degree(self.clean_quotes(node_id))         return result      async def edge_degree(self, src_id: str, tgt_id: str) -&gt; int:         return self._graph.degree(self.clean_quotes(src_id)) + self._graph.degree(             self.clean_quotes(tgt_id)         )      async def get_node(self, node_id: str) -&gt; dict | None:         result = self._graph.get_node_data(self.clean_quotes(node_id))         return result      async def get_edge(self, source_node_id: str, target_node_id: str) -&gt; dict | None:         result = self._graph.get_edge_data(             self.clean_quotes(source_node_id), self.clean_quotes(target_node_id)         )         return result      async def get_node_edges(self, source_node_id: str) -&gt; list[tuple[str, str]] | None:         source_node_id = self.clean_quotes(source_node_id)         if self._graph.has_node(source_node_id):             edges = self._graph.get_node_edges(source_node_id)             return list(edges)         return None      async def upsert_node(self, node_id: str, node_data: dict[str, str]):         node_id = self.clean_quotes(node_id)         self._graph.add_node(node_id, **node_data)      async def upsert_edge(         self, source_node_id: str, target_node_id: str, edge_data: dict[str, str]     ):         source_node_id = self.clean_quotes(source_node_id)         target_node_id = self.clean_quotes(target_node_id)         self._graph.add_edge(source_node_id, target_node_id, **edge_data)      async def delete_node(self, node_id: str):         if self._graph.has_node(node_id):             self._graph.remove_node(node_id)             logger.info(f\"Node {node_id} deleted from the graph.\")         else:             logger.warning(f\"Node {node_id} not found in the graph for deletion.\")      async def embed_nodes(self, algorithm: str) -&gt; tuple[np.ndarray, list[str]]:         return np.array([]), [] <p>This code defines a <code>TigerGraphStorage</code> class that implements the <code>BaseGraphStorage</code> interface for graph storage and retrieval using TigerGraphX, a Python library for interacting with TigerGraph databases.</p> <p>Key highlights of this implementation include:</p> <ol> <li><p>Graph Initialization</p> <ul> <li>An undirected homogeneous graph (<code>UndiGraph</code>) is initialized. This graph type supports only one type of node and edge, making it similar to NetworkX's undirected graph.</li> <li>TigerGraph\u2019s schema-based nature requires a graph schema definition with specific attributes for nodes and edges. For instance:<ul> <li>Node attributes: <code>id</code>, <code>entity_type</code>, <code>description</code>, <code>source_id</code></li> <li>Edge attributes: <code>weight</code>, <code>description</code>, <code>keywords</code>, <code>source_id</code></li> </ul> </li> </ul> </li> <li><p>TigerGraphX Interfaces</p> <ul> <li>TigerGraphX provides user-friendly interfaces, very similar to NetworkX, which simplify operations like:<ul> <li>Node Operations: <code>has_node</code>, <code>add_node</code>, <code>remove_node</code>, <code>get_node_data</code></li> <li>Edge Operations: <code>has_edge</code>, <code>add_edge</code>, <code>get_edge_data</code>, <code>get_node_edges</code></li> <li>Degree Calculation: <code>degree</code> for nodes and edges.</li> </ul> </li> </ul> </li> <li><p>Key Methods</p> <ul> <li>Storage Operations:<ul> <li><code>upsert_node</code>: Inserts or updates a node with its data.</li> <li><code>upsert_edge</code>: Inserts or updates an edge between two nodes.</li> <li><code>delete_node</code>: Deletes a node if it exists.</li> </ul> </li> <li>Data Retrieval:<ul> <li><code>get_node</code>: Retrieves data for a specific node.</li> <li><code>get_edge</code>: Retrieves data for a specific edge.</li> <li><code>get_node_edges</code>: Retrieves all edges for a given node.</li> </ul> </li> <li>Graph Metrics:<ul> <li><code>node_degree</code>: Returns the degree of a node.</li> <li><code>edge_degree</code>: Calculates combined degrees of two nodes.</li> </ul> </li> </ul> </li> <li><p>Additional Notes</p> <ul> <li>The <code>clean_quotes</code> method ensures clean input values by stripping leading and trailing quotes from strings.</li> <li>TigerGraphX goes beyond NetworkX\u2019s capabilities by supporting heterogeneous graphs (graphs with multiple types of nodes and edges) using the <code>Graph</code> class, in addition to undirected (<code>UndiGraph</code>) and directed graphs (<code>DiGraph</code>).</li> </ul> </li> </ol> In\u00a0[3]: Copied! <pre>from lightrag import LightRAG\n\n\n# Define a subclass to include your custom graph storage in the storage mapping\nclass CustomLightRAG(LightRAG):\n    def _get_storage_class(self):\n        # Extend the default storage mapping with your custom storage\n        base_mapping = super()._get_storage_class()\n        base_mapping[\"TigerGraphStorage\"] = TigerGraphStorage\n        return base_mapping\n</pre> from lightrag import LightRAG   # Define a subclass to include your custom graph storage in the storage mapping class CustomLightRAG(LightRAG):     def _get_storage_class(self):         # Extend the default storage mapping with your custom storage         base_mapping = super()._get_storage_class()         base_mapping[\"TigerGraphStorage\"] = TigerGraphStorage         return base_mapping In\u00a0[6]: Copied! <pre>import nest_asyncio\n# Use the nest_asyncio package to allow running nested event loops in Jupyter Notebook without conflicts.\nnest_asyncio.apply()\n\nworking_dir = \"../../applications/lightrag/data\"\n\ncustom_rag = CustomLightRAG(\n    working_dir=working_dir,\n    graph_storage=\"TigerGraphStorage\",\n)\n\nwith open(working_dir + \"/input/fin.txt\") as f:\n    custom_rag.insert(f.read())\n</pre> import nest_asyncio # Use the nest_asyncio package to allow running nested event loops in Jupyter Notebook without conflicts. nest_asyncio.apply()  working_dir = \"../../applications/lightrag/data\"  custom_rag = CustomLightRAG(     working_dir=working_dir,     graph_storage=\"TigerGraphStorage\", )  with open(working_dir + \"/input/fin.txt\") as f:     custom_rag.insert(f.read()) <pre>2024-12-17 22:21:20,312 - lightrag - INFO - Logger initialized for working directory: ../../applications/lightrag/data\n2024-12-17 22:21:20,315 - lightrag - INFO - Load KV llm_response_cache with 0 data\n2024-12-17 22:21:20,318 - lightrag - INFO - Load KV full_docs with 0 data\n2024-12-17 22:21:20,320 - lightrag - INFO - Load KV text_chunks with 0 data\n2024-12-17 22:21:20,322 - lightrag - INFO - TigerGraph connection configuration retrieved successfully.\n2024-12-17 22:21:20,386 - lightrag - INFO - Undirected graph initialized successfully with graph_name 'LightRAG'.\n2024-12-17 22:21:20,387 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_entities.json'} 0 data\n2024-12-17 22:21:20,389 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_relationships.json'} 0 data\n2024-12-17 22:21:20,390 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_chunks.json'} 0 data\n2024-12-17 22:21:20,401 - lightrag - INFO - [New Docs] inserting 1 docs\n</pre> <pre>Chunking documents: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 18.96doc/s]</pre> <pre>2024-12-17 22:21:20,456 - lightrag - INFO - [New Chunks] inserting 46 chunks\n2024-12-17 22:21:20,457 - lightrag - INFO - Inserting 46 vectors to chunks\n</pre> <pre>\nGenerating embeddings:   0%|                                                                                                                         | 0/2 [00:00&lt;?, ?batch/s]</pre> <pre>2024-12-17 22:21:21,645 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:21,957 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>Generating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.04batch/s]</pre> <pre>2024-12-17 22:21:22,587 - lightrag - INFO - [Entity Extraction]...\n</pre> <pre>\n\nenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.07s/batch]</pre> <pre>2024-12-17 22:21:31,254 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:32,446 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:33,175 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:33,260 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:33,452 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:33,519 - openai._base_client - INFO - Retrying request to /chat/completions in 0.432498 seconds\n2024-12-17 22:21:33,881 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:34,010 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:34,094 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:34,153 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:34,437 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:35,645 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:36,193 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:36,368 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:37,039 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:37,099 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:39,156 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:39,479 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2819 Processed 1 chunks, 18 entities(duplicated), 5 relations(duplicated)</pre> <pre>\nracting entities from chunks:   2%|\u2588\u2588\u258f                                                                                                   | 1/46 [00:16&lt;12:41, 16.91s/chunk]</pre> <pre>2024-12-17 22:21:44,110 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2839 Processed 2 chunks, 35 entities(duplicated), 11 relations(duplicated)</pre> <pre>\nracting entities from chunks:   4%|\u2588\u2588\u2588\u2588\u258d                                                                                                 | 2/46 [00:21&lt;07:06,  9.69s/chunk]</pre> <pre>2024-12-17 22:21:46,600 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2838 Processed 3 chunks, 58 entities(duplicated), 11 relations(duplicated)</pre> <pre>\nracting entities from chunks:   7%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                               | 3/46 [00:24&lt;04:35,  6.40s/chunk]</pre> <pre>2024-12-17 22:21:47,774 - openai._base_client - INFO - Retrying request to /chat/completions in 0.417796 seconds\n2024-12-17 22:21:48,376 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u283c Processed 4 chunks, 79 entities(duplicated), 15 relations(duplicated)</pre> <pre>\nracting entities from chunks:   9%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                             | 4/46 [00:25&lt;03:12,  4.58s/chunk]</pre> <pre>2024-12-17 22:21:48,597 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2834 Processed 5 chunks, 107 entities(duplicated), 21 relations(duplicated)</pre> <pre>\nracting entities from chunks:  11%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                           | 5/46 [00:26&lt;02:03,  3.00s/chunk]</pre> <pre>2024-12-17 22:21:49,309 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2826 Processed 6 chunks, 125 entities(duplicated), 33 relations(duplicated)</pre> <pre>\nracting entities from chunks:  13%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                        | 6/46 [00:26&lt;01:29,  2.23s/chunk]</pre> <pre>2024-12-17 22:21:49,332 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:50,339 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2807 Processed 8 chunks, 173 entities(duplicated), 47 relations(duplicated)</pre> <pre>\nracting entities from chunks:  17%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                    | 8/46 [00:27&lt;00:52,  1.38s/chunk]</pre> <pre>2024-12-17 22:21:51,242 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:51,462 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u280f Processed 9 chunks, 190 entities(duplicated), 61 relations(duplicated)</pre> <pre>\nracting entities from chunks:  20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                  | 9/46 [00:28&lt;00:48,  1.32s/chunk]</pre> <pre>2024-12-17 22:21:52,500 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u280b Processed 10 chunks, 220 entities(duplicated), 68 relations(duplicated)</pre> <pre>\nracting entities from chunks:  22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                               | 10/46 [00:29&lt;00:44,  1.24s/chunk]</pre> <pre>2024-12-17 22:21:53,034 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2819 Processed 11 chunks, 248 entities(duplicated), 76 relations(duplicated)</pre> <pre>\nracting entities from chunks:  24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                            | 11/46 [00:30&lt;00:36,  1.04s/chunk]</pre> <pre>2024-12-17 22:21:53,057 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:21:54,463 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2838 Processed 13 chunks, 292 entities(duplicated), 101 relations(duplicated)</pre> <pre>\nracting entities from chunks:  28%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                        | 13/46 [00:31&lt;00:29,  1.11chunk/s]</pre> <pre>2024-12-17 22:21:55,590 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u283c Processed 14 chunks, 326 entities(duplicated), 107 relations(duplicated)</pre> <pre>\nracting entities from chunks:  30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                      | 14/46 [00:33&lt;00:30,  1.05chunk/s]</pre> <pre>2024-12-17 22:21:56,337 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:01,015 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:02,055 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:02,278 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2834 Processed 15 chunks, 348 entities(duplicated), 112 relations(duplicated)</pre> <pre>\nracting entities from chunks:  33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                    | 15/46 [00:39&lt;01:15,  2.42s/chunk]</pre> <pre>2024-12-17 22:22:03,845 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:04,893 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2826 Processed 16 chunks, 370 entities(duplicated), 118 relations(duplicated)</pre> <pre>\nracting entities from chunks:  35%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                 | 16/46 [00:42&lt;01:14,  2.47s/chunk]</pre> <pre>2024-12-17 22:22:05,825 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:05,888 - openai._base_client - INFO - Retrying request to /chat/completions in 0.469987 seconds\n2024-12-17 22:22:06,672 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:06,808 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:07,741 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:07,961 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:09,208 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:10,030 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:11,257 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:12,587 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2827 Processed 17 chunks, 394 entities(duplicated), 124 relations(duplicated)</pre> <pre>\nracting entities from chunks:  37%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                               | 17/46 [00:50&lt;01:53,  3.92s/chunk]</pre> <pre>2024-12-17 22:22:12,667 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:15,658 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2807 Processed 18 chunks, 410 entities(duplicated), 136 relations(duplicated)</pre> <pre>\nracting entities from chunks:  39%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                             | 18/46 [00:53&lt;01:42,  3.68s/chunk]</pre> <pre>2024-12-17 22:22:15,901 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u280f Processed 19 chunks, 428 entities(duplicated), 150 relations(duplicated)</pre> <pre>\nracting entities from chunks:  41%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                           | 19/46 [00:53&lt;01:12,  2.69s/chunk]</pre> <pre>2024-12-17 22:22:16,102 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:18,223 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u280b Processed 20 chunks, 445 entities(duplicated), 162 relations(duplicated)</pre> <pre>\nracting entities from chunks:  43%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                         | 20/46 [00:55&lt;01:07,  2.58s/chunk]</pre> <pre>2024-12-17 22:22:19,361 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:19,492 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2819 Processed 21 chunks, 468 entities(duplicated), 168 relations(duplicated)</pre> <pre>\nracting entities from chunks:  46%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                       | 21/46 [00:56&lt;00:54,  2.19s/chunk]</pre> <pre>2024-12-17 22:22:20,260 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2839 Processed 22 chunks, 490 entities(duplicated), 174 relations(duplicated)</pre> <pre>\nracting entities from chunks:  48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                    | 22/46 [00:57&lt;00:42,  1.77s/chunk]</pre> <pre>2024-12-17 22:22:20,868 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2838 Processed 23 chunks, 514 entities(duplicated), 185 relations(duplicated)</pre> <pre>\nracting entities from chunks:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                  | 23/46 [00:58&lt;00:32,  1.43s/chunk]</pre> <pre>2024-12-17 22:22:21,142 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u283c Processed 24 chunks, 531 entities(duplicated), 196 relations(duplicated)</pre> <pre>\nracting entities from chunks:  52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                | 24/46 [00:58&lt;00:23,  1.08s/chunk]</pre> <pre>2024-12-17 22:22:21,517 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:21,624 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2834 Processed 25 chunks, 550 entities(duplicated), 211 relations(duplicated)</pre> <pre>\nracting entities from chunks:  54%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                              | 25/46 [00:59&lt;00:19,  1.11chunk/s]</pre> <pre>2024-12-17 22:22:21,859 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2826 Processed 26 chunks, 579 entities(duplicated), 222 relations(duplicated)</pre> <pre>\nracting entities from chunks:  57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                            | 26/46 [00:59&lt;00:14,  1.42chunk/s]</pre> <pre>2024-12-17 22:22:27,440 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2827 Processed 27 chunks, 602 entities(duplicated), 244 relations(duplicated)</pre> <pre>\nracting entities from chunks:  59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                         | 27/46 [01:04&lt;00:41,  2.16s/chunk]</pre> <pre>2024-12-17 22:22:29,177 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:31,430 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:31,779 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:31,878 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:32,454 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2807 Processed 28 chunks, 621 entities(duplicated), 247 relations(duplicated)</pre> <pre>\nracting entities from chunks:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                       | 28/46 [01:09&lt;00:54,  3.02s/chunk]</pre> <pre>2024-12-17 22:22:34,606 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u280f Processed 29 chunks, 645 entities(duplicated), 252 relations(duplicated)</pre> <pre>\nracting entities from chunks:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                     | 29/46 [01:12&lt;00:46,  2.76s/chunk]</pre> <pre>2024-12-17 22:22:36,038 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:36,236 - openai._base_client - INFO - Retrying request to /chat/completions in 0.488199 seconds\n2024-12-17 22:22:36,343 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u280b Processed 30 chunks, 678 entities(duplicated), 269 relations(duplicated)</pre> <pre>\nracting entities from chunks:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                   | 30/46 [01:13&lt;00:39,  2.45s/chunk]</pre> <pre>2024-12-17 22:22:36,933 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:37,888 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:37,944 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:38,356 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2819 Processed 31 chunks, 721 entities(duplicated), 278 relations(duplicated)</pre> <pre>\nracting entities from chunks:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                 | 31/46 [01:15&lt;00:34,  2.32s/chunk]</pre> <pre>2024-12-17 22:22:39,420 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2839 Processed 32 chunks, 763 entities(duplicated), 285 relations(duplicated)</pre> <pre>\nracting entities from chunks:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                              | 32/46 [01:16&lt;00:27,  1.94s/chunk]</pre> <pre>2024-12-17 22:22:40,630 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2838 Processed 33 chunks, 784 entities(duplicated), 291 relations(duplicated)</pre> <pre>\nracting entities from chunks:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                            | 33/46 [01:18&lt;00:22,  1.72s/chunk]</pre> <pre>2024-12-17 22:22:43,002 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:45,972 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u283c Processed 34 chunks, 809 entities(duplicated), 300 relations(duplicated)</pre> <pre>\nracting entities from chunks:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                          | 34/46 [01:23&lt;00:33,  2.81s/chunk]</pre> <pre>2024-12-17 22:22:46,004 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:46,471 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2834 Processed 35 chunks, 832 entities(duplicated), 305 relations(duplicated)</pre> <pre>\nracting entities from chunks:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                        | 35/46 [01:23&lt;00:23,  2.12s/chunk]</pre> <pre>2024-12-17 22:22:46,999 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:47,608 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:48,329 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2826 Processed 36 chunks, 850 entities(duplicated), 322 relations(duplicated)</pre> <pre>\nracting entities from chunks:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                      | 36/46 [01:25&lt;00:20,  2.04s/chunk]</pre> <pre>2024-12-17 22:22:49,494 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:22:50,553 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2827 Processed 37 chunks, 876 entities(duplicated), 335 relations(duplicated)</pre> <pre>\nracting entities from chunks:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                   | 37/46 [01:27&lt;00:18,  2.09s/chunk]</pre> <pre>2024-12-17 22:22:52,833 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2807 Processed 38 chunks, 895 entities(duplicated), 351 relations(duplicated)</pre> <pre>\nracting entities from chunks:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                 | 38/46 [01:30&lt;00:17,  2.15s/chunk]</pre> <pre>2024-12-17 22:22:52,852 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u280f Processed 39 chunks, 917 entities(duplicated), 355 relations(duplicated)</pre> <pre>\nracting entities from chunks:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b               | 39/46 [01:30&lt;00:10,  1.54s/chunk]</pre> <pre>2024-12-17 22:22:54,576 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u280b Processed 40 chunks, 942 entities(duplicated), 363 relations(duplicated)</pre> <pre>\nracting entities from chunks:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a             | 40/46 [01:31&lt;00:09,  1.57s/chunk]</pre> <pre>2024-12-17 22:22:57,504 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2819 Processed 41 chunks, 966 entities(duplicated), 370 relations(duplicated)</pre> <pre>\nracting entities from chunks:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588           | 41/46 [01:34&lt;00:09,  1.98s/chunk]</pre> <pre>2024-12-17 22:23:00,411 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2839 Processed 42 chunks, 982 entities(duplicated), 384 relations(duplicated)</pre> <pre>\nracting entities from chunks:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f        | 42/46 [01:37&lt;00:09,  2.25s/chunk]</pre> <pre>2024-12-17 22:23:04,304 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2838 Processed 43 chunks, 1014 entities(duplicated), 389 relations(duplicated)</pre> <pre>\nracting entities from chunks:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d      | 43/46 [01:41&lt;00:08,  2.75s/chunk]</pre> <pre>2024-12-17 22:23:06,248 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u283c Processed 44 chunks, 1039 entities(duplicated), 396 relations(duplicated)</pre> <pre>\nracting entities from chunks:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c    | 44/46 [01:43&lt;00:05,  2.51s/chunk]</pre> <pre>2024-12-17 22:23:09,627 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2834 Processed 45 chunks, 1092 entities(duplicated), 412 relations(duplicated)</pre> <pre>\nracting entities from chunks:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 45/46 [01:47&lt;00:02,  2.77s/chunk]</pre> <pre>2024-12-17 22:24:09,691 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n\u2826 Processed 46 chunks, 1342 entities(duplicated), 417 relations(duplicated)</pre> <pre>\nExtracting entities from chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46/46 [02:47&lt;00:00,  3.64s/chunk]</pre> <pre>2024-12-17 22:24:09,861 - lightrag - INFO - Inserting entities into storage...\n</pre> <pre>\nInserting entities:   0%|                                                                                                               | 1/938 [00:14&lt;3:49:23, 14.69s/entity]</pre> <pre>2024-12-17 22:24:28,475 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n</pre> <pre>Inserting entities: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 937/938 [00:18&lt;00:00, 65.82entity/s]</pre> <pre>2024-12-17 22:24:29,908 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n</pre> <pre>Inserting entities: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 938/938 [00:20&lt;00:00, 46.67entity/s]</pre> <pre>2024-12-17 22:24:29,964 - lightrag - INFO - Inserting relationships into storage...\n</pre> <pre>\nInserting relationships: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 368/368 [00:13&lt;00:00, 27.27relationship/s]</pre> <pre>2024-12-17 22:24:43,463 - lightrag - INFO - Inserting 938 vectors to entities\n</pre> <pre>\nGenerating embeddings:   0%|                                                                                                                        | 0/30 [00:00&lt;?, ?batch/s]</pre> <pre>2024-12-17 22:24:44,858 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:44,872 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:44,946 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:44,967 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:44,970 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:44,974 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:45,028 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:45,080 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:45,128 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:45,142 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:45,157 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:45,166 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:45,172 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:45,380 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>Generating embeddings:   3%|\u2588\u2588\u2588\u258b                                                                                                            | 1/30 [00:02&lt;00:59,  2.04s/batch]</pre> <pre>2024-12-17 22:24:45,564 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:45,568 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>Generating embeddings:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                       | 15/30 [00:04&lt;00:01,  8.47batch/s]</pre> <pre>2024-12-17 22:24:48,852 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:48,861 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:48,889 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:48,951 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:48,983 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:48,990 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:49,022 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:49,032 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:49,114 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>Generating embeddings:  57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                | 17/30 [00:05&lt;00:04,  2.94batch/s]</pre> <pre>2024-12-17 22:24:49,340 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>Generating embeddings:  60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                            | 18/30 [00:05&lt;00:03,  3.44batch/s]</pre> <pre>2024-12-17 22:24:49,443 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>Generating embeddings:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                 | 21/30 [00:06&lt;00:01,  5.31batch/s]</pre> <pre>2024-12-17 22:24:49,831 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>Generating embeddings:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                          | 23/30 [00:06&lt;00:01,  6.42batch/s]</pre> <pre>2024-12-17 22:24:50,028 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>Generating embeddings:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c       | 28/30 [00:06&lt;00:00,  8.49batch/s]</pre> <pre>2024-12-17 22:24:51,049 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>Generating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:08&lt;00:00,  3.26batch/s]</pre> <pre>2024-12-17 22:24:51,801 - lightrag - INFO - Inserting 368 vectors to relationships\n</pre> <pre>\n\nenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:08&lt;00:00,  3.57batch/s]</pre> <pre>2024-12-17 22:24:52,089 - openai._base_client - INFO - Retrying request to /embeddings in 0.492858 seconds\n2024-12-17 22:24:52,707 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:52,792 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:52,854 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:53,002 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:53,042 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:53,056 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:53,092 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:53,108 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:53,122 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:24:53,205 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>\n\n\nerating embeddings:  25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                    | 3/12 [00:01&lt;00:03,  2.35batch/s]</pre> <pre>2024-12-17 22:24:53,552 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>\n\n\n\nerating embeddings:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                     | 8/12 [00:02&lt;00:00,  6.87batch/s]</pre> <pre>2024-12-17 22:24:54,041 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n</pre> <pre>\n\n\n\nerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:02&lt;00:00,  7.79batch/s]</pre> In\u00a0[7]: Copied! <pre>from lightrag import QueryParam\n\ncustom_rag = CustomLightRAG(\n    working_dir=working_dir,\n    graph_storage=\"TigerGraphStorage\",\n)\n\nquery = \"What is the overall financial health of the company?\"\n\nresult = custom_rag.query(query=query, param=QueryParam(mode=\"hybrid\"))\n\nprint(\"------------------- Query Result:  -------------------\")\nprint(result)\n</pre> from lightrag import QueryParam  custom_rag = CustomLightRAG(     working_dir=working_dir,     graph_storage=\"TigerGraphStorage\", )  query = \"What is the overall financial health of the company?\"  result = custom_rag.query(query=query, param=QueryParam(mode=\"hybrid\"))  print(\"------------------- Query Result:  -------------------\") print(result) <pre>2024-12-17 22:27:48,142 - lightrag - INFO - Logger initialized for working directory: ../../applications/lightrag/data\n2024-12-17 22:27:48,145 - lightrag - INFO - Load KV llm_response_cache with 0 data\n2024-12-17 22:27:48,148 - lightrag - INFO - Load KV full_docs with 1 data\n2024-12-17 22:27:48,153 - lightrag - INFO - Load KV text_chunks with 46 data\n2024-12-17 22:27:48,155 - lightrag - INFO - TigerGraph connection configuration retrieved successfully.\n2024-12-17 22:27:48,248 - lightrag - INFO - Undirected graph initialized successfully with graph_name 'LightRAG'.\n</pre> <pre>Generating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [02:56&lt;00:00, 14.71s/batch]</pre> <pre>2024-12-17 22:27:48,328 - nano-vectordb - INFO - Load (938, 1536) data\n2024-12-17 22:27:48,332 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_entities.json'} 938 data\n2024-12-17 22:27:48,376 - nano-vectordb - INFO - Load (368, 1536) data\n2024-12-17 22:27:48,379 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_relationships.json'} 368 data\n2024-12-17 22:27:48,382 - nano-vectordb - INFO - Load (46, 1536) data\n2024-12-17 22:27:48,382 - nano-vectordb - INFO - Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../../applications/lightrag/data/vdb_chunks.json'} 46 data\n</pre> <pre>\n</pre> <pre>2024-12-17 22:27:49,499 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2024-12-17 22:27:49,500 - lightrag - INFO - kw_prompt result:\n{\n  \"high_level_keywords\": [\"Financial health\", \"Company performance\", \"Economic assessment\"],\n  \"low_level_keywords\": [\"Revenue\", \"Expenses\", \"Profit margins\", \"Assets\", \"Liabilities\"]\n}\n2024-12-17 22:27:50,009 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:27:52,889 - lightrag - INFO - Local query uses 60 entites, 35 relations, 3 text units\n2024-12-17 22:27:53,582 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-12-17 22:27:54,039 - lightrag - WARNING - Some edges are missing, maybe the storage is damaged\n2024-12-17 22:27:55,274 - lightrag - INFO - Global query uses 48 entites, 58 relations, 3 text units\n2024-12-17 22:28:08,736 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n------------------- Query Result:  -------------------\n## Financial Overview of CytoSorbents Corporation\n\nCytoSorbents Corporation is engaged in the development and commercialization of medical technologies, particularly focusing on its flagship product, CytoSorb, which is a blood purification device. The company's financial health can be assessed through various aspects, including revenue generation, funding activities, and market strategies.\n\n### Funding and Capital Raising\n\nThe company has actively pursued multiple avenues of funding to support its operations and research initiatives. Notably, in July 2020, CytoSorbents closed an underwritten public offering that raised approximately $57.5 million in gross proceeds. After accounting for underwriting fees and other expenses, the net proceeds amounted to about $53.8 million. This influx of capital has been crucial in enabling the company to fund research and development efforts, expand manufacturing capabilities, and enhance its product offerings.\n\nThe corporation has also secured significant non-dilutive funding from various governmental organizations, amounting to approximately $28.4 million since 2012. This includes grants and contracts from notable entities such as DARPA, the U.S. Army, and the National Heart, Lung, and Blood Institute. These diverse funding sources underscore the company's commitment to innovation and its capacity to attract financial support for its projects.\n\n### Revenue Generation and Market Presence\n\nCytoSorbents\u2019 presence in the European market, especially in Germany, reflects a growing demand for its products. The company has achieved dedicated reimbursement codes in Germany since January 2017, facilitating better reimbursement for its CytoSorb device in medical procedures. The recognition and acceptance of CytoSorb in over 40,000 treatments highlight its integration into critical care practices within Europe.\n\nIn addition, the company's attempts to penetrate the U.S. market are vital for its long-term growth. While CytoSorb is not yet approved for widespread use in the U.S., it received FDA Emergency Use Authorization in April 2020 for COVID-19 treatment applications. This regulatory achievement has opened potential revenue avenues from a significant market, provided that further approvals for various clinical applications can be secured.\n\n### Cost Management and Financial Risks\n\nDespite the positive outlook from funding and revenue generation, the company faces financial pressures stemming from operational challenges, particularly in terms of compliance with regulatory frameworks. The potential impact of the American Taxpayer Relief Act may create uncertainty regarding government funding, which poses a risk to the company's financial stability. Furthermore, the company's operations may be affected by sequestration and potential automatic cuts in federal spending.\n\nRoyalty obligations stemming from agreements involving the use of patented technology also impose additional costs, with royalty payments amounting to approximately $1.17 million for the year ended December 31, 2020. Such obligations are indicative of the financial commitments related to intellectual property and product development.\n\n### Conclusion\n\nOverall, CytoSorbents Corporation demonstrates a robust financial position characterized by diverse funding avenues, a solid presence in international markets, and efforts to penetrate the U.S. healthcare landscape. However, it faces challenges related to regulatory compliance and the financial implications of obligations arising from its operational frameworks. The company's ability to navigate these challenges while leveraging its technological advancements will be crucial for sustaining its growth trajectory and enhancing financial health moving forward.\n</pre>"},{"location":"graphrag/lightrag/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before proceeding, ensure you\u2019ve completed the installation and setup steps outlined in the Installation Guide, including:</p> <ul> <li><p>Setting up Python and TigerGraph. For more details, refer to the Requirements section.</p> </li> <li><p>Install TigerGraphX along with its development dependencies. For more details, refer to the Development Installation section.</p> </li> <li><p>Set the environment variables <code>TG_HOST</code>, <code>TG_USERNAME</code>, and <code>TG_PASSWORD</code>, which are required to connect to the TigerGraph server, as well as <code>OPENAI_API_KEY</code> for connecting to OpenAI. Use a command like the following to set these variables:</p> <pre>export TG_HOST=https://127.0.0.1\n</pre> </li> </ul>"},{"location":"graphrag/lightrag/#implement-graph-storage-with-tigergraphx","title":"Implement Graph Storage with TigerGraphX\u00b6","text":"<p>In LightRAG, the storage layers are abstracted into components such as graph storage, key-value storage, and vector storage. You can refer to the base classes BaseGraphStorage, BaseVectorStorage, and BaseKVStorage in the source code.</p> <p>In this section, we will demonstrate how to use TigerGraphX to implement the <code>BaseGraphStorage</code> class for storing and retrieving data in TigerGraph.</p>"},{"location":"graphrag/lightrag/#integrating-custom-graph-storage-with-lightrag","title":"Integrating Custom Graph Storage with LightRAG\u00b6","text":"<p>After defining the <code>TigerGraphStorage</code> class, we integrate it into LightRAG. By subclassing LightRAG and extending its storage mapping, you can easily replace or augment the default storage backends with your custom solution.</p> <p>While modifying the LightRAG source code is another option, this example demonstrates how to achieve the integration without altering the original source code.</p> <p>Below is the code for creating a <code>CustomLightRAG</code> class that incorporates <code>TigerGraphStorage</code> into its storage mapping.</p>"},{"location":"graphrag/lightrag/#indexing","title":"Indexing\u00b6","text":""},{"location":"graphrag/lightrag/#data-preparation","title":"Data Preparation\u00b6","text":""},{"location":"graphrag/lightrag/#set-up-working-directory","title":"Set Up Working Directory\u00b6","text":"<p>Create a folder to serve as the working directory. For this demo, we will use <code>applications/lightrag/data</code>.</p> <p>Next, create an <code>input</code> folder inside the <code>data</code> directory to store the documents you want to index:</p> <pre>mkdir -p applications/lightrag/data/input\n</pre>"},{"location":"graphrag/lightrag/#add-documents-to-the-input-folder","title":"Add Documents to the Input Folder\u00b6","text":"<p>Copy your documents (e.g., <code>fin.txt</code>) into the <code>applications/lightrag/data/input</code> folder.</p>"},{"location":"graphrag/lightrag/#indexing","title":"Indexing\u00b6","text":"<p>The following code sets up a working directory and demonstrates how to index a given document using LightRAG.</p>"},{"location":"graphrag/lightrag/#querying","title":"Querying\u00b6","text":"<p>The following code demonstrates how to perform a query in LightRAG using the TigerGraph graph storage implementation.</p>"},{"location":"graphrag/msft_graphrag_1/","title":"Supporting Microsoft\u2019s GraphRAG: Part 1","text":"<p>Microsoft's GraphRAG is a method for creating structured knowledge graphs from raw text, enhancing Retrieval Augmented Generation (RAG) tasks. By organizing information hierarchically, it enables more efficient data retrieval and summarization.</p>"},{"location":"graphrag/msft_graphrag_1/#what-youll-learn-in-this-guide","title":"What You\u2019ll Learn in This Guide","text":"<ul> <li>Indexing: Utilize Microsoft's GraphRAG to convert unstructured documents into Parquet files.</li> <li>Data Preprocessing: Learn how to use utility methods provided by TigerGraphX to transform Parquet files into CSV files compatible with TigerGraph.</li> <li>Schema Design: Understand how to design a graph schema for storing your data.</li> <li>Data Loading: Map the CSV files to the graph schema and load them into TigerGraph seamlessly.</li> </ul>"},{"location":"graphrag/msft_graphrag_1/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you\u2019ve completed the installation and setup steps outlined in the Installation Guide, including:</p> <ul> <li>Setting up Python and TigerGraph. For more details, refer to the Requirements section.</li> <li>Install TigerGraphX along with its development dependencies. For more details, refer to the Development Installation section.</li> </ul>"},{"location":"graphrag/msft_graphrag_1/#utilize-microsoft-graphrag-for-indexing","title":"Utilize Microsoft GraphRAG for Indexing","text":"<p>The indexing process transforms raw documents into structured data using Microsoft\u2019s GraphRAG. Follow these steps to prepare your data:</p>"},{"location":"graphrag/msft_graphrag_1/#data-preparation","title":"Data Preparation","text":""},{"location":"graphrag/msft_graphrag_1/#create-an-input-folder","title":"Create an Input Folder","text":"<p>Create an <code>input</code> folder in the <code>data</code> directory under <code>applications/msft_graphrag</code> to store the documents you want to index. You can specify a different directory by replacing <code>data</code> with your desired path.</p> <pre><code>mkdir -p data/input\n</code></pre>"},{"location":"graphrag/msft_graphrag_1/#add-documents-to-the-input-folder","title":"Add Documents to the Input Folder","text":"<p>Copy your documents (e.g., <code>fin.txt</code>) into the <code>data/input</code> folder.</p>"},{"location":"graphrag/msft_graphrag_1/#initialization","title":"Initialization","text":"<p>Initialize the indexing system in the <code>data</code> directory.</p> <pre><code>python3 -m graphrag init --root data\n</code></pre>"},{"location":"graphrag/msft_graphrag_1/#set-up-openai-api-key","title":"Set Up OpenAI API Key","text":"<p>GraphRAG requires an OpenAI API key. To configure it:</p> <ol> <li>Open the <code>.env</code> file in the <code>data</code> directory:    <pre><code>vi data/.env\n</code></pre></li> <li>Add your API key:    <pre><code>GRAPHRAG_API_KEY=&lt;Your OpenAI API Key&gt;\n</code></pre></li> </ol>"},{"location":"graphrag/msft_graphrag_1/#optional-switch-to-a-cost-effective-model","title":"Optional: Switch to a Cost-Effective Model","text":"<p>GraphRAG uses the <code>gpt-4-turbo-preview</code> model by default. To reduce costs, switch to the <code>gpt-4o-mini</code> model by editing the <code>settings.yaml</code> file in the <code>data</code> directory:</p> <pre><code>llm:\n  api_key: ${GRAPHRAG_API_KEY}\n  type: openai_chat # or azure_openai_chat\n  model: gpt-4o-mini # Use a cost-effective model\n</code></pre>"},{"location":"graphrag/msft_graphrag_1/#indexing","title":"Indexing","text":"<p>Run the indexing process to convert documents into structured data. This step uses LLMs and may take several minutes depending on the dataset size and hardware.</p> <pre><code>python3 -m graphrag index --no-cache --root data\n</code></pre>"},{"location":"graphrag/msft_graphrag_1/#utilize-tigergraphx-for-data-preprocessing","title":"Utilize TigerGraphX for Data Preprocessing","text":"<p>Transform the structured Parquet files generated by GraphRAG into CSV files that TigerGraph can import.</p>"},{"location":"graphrag/msft_graphrag_1/#convert-parquet-to-csv","title":"Convert Parquet to CSV","text":"<p>Run the script below to convert Parquet files into TigerGraph-compatible CSV files. You can find the Python script here.</p> <pre><code>python3 data_import/convert_parquet_to_tg_csv.py \\\n--input_dir data/output \\\n--output_dir data/tg_csv\n</code></pre>"},{"location":"graphrag/msft_graphrag_1/#transfer-csv-files-to-tigergraph-server","title":"Transfer CSV Files to TigerGraph Server","text":"<p>Transfer the generated CSV files to your TigerGraph server. Use the following command, replacing <code>username</code> and <code>tigergraph-server</code> with your server credentials:</p> <pre><code>scp data/tg_csv/* username@tigergraph-server:/home/tigergraph/data/graphrag\n</code></pre>"},{"location":"graphrag/msft_graphrag_1/#create-a-graph","title":"Create a Graph","text":"<p>TigerGraph is a schema-based database, which requires defining a schema to structure your graph. This schema specifies the graph name, nodes (vertices), edges (relationships), and their respective attributes.</p> <p>Now, let's define the schema using Python. You can execute the following code in a Python shell or Jupyter Notebook. To access the original <code>.ipynb</code> file, download it from msft_graphrag_1_1.ipynb.</p>"},{"location":"graphrag/msft_graphrag_1/#define-a-graph-schema","title":"Define a Graph Schema","text":"<p>In this example, we will initialize a graph using a schema defined in a YAML file. The schema structure is represented visually in the following image.</p> <p></p> <p>First, convert the YAML file into a graph schema using the <code>GraphSchema.ensure_config</code> method. <pre><code>from tigergraphx import Graph, GraphSchema, LoadingJobConfig, TigerGraphConnectionConfig\nresource_dir = \"../../applications/msft_graphrag/query/resources/\"\nschema_path = resource_dir + \"graph_schema.yaml\"\ngraph_schema=GraphSchema.ensure_config(schema_path)\n</code></pre></p>"},{"location":"graphrag/msft_graphrag_1/#define-the-tigergraph-connection-configuration","title":"Define the TigerGraph Connection Configuration","text":"<p>In addition to defining the schema, a connection configuration is necessary to establish communication with the TigerGraph server.</p> <pre><code>connection = TigerGraphConnectionConfig.ensure_config({\n    \"host\": \"http://127.0.0.1\",\n    \"user_name\": \"tigergraph\",\n    \"password\": \"tigergraph\",\n})\n</code></pre>"},{"location":"graphrag/msft_graphrag_1/#create-a-graph_1","title":"Create a Graph","text":"<p>Running the following command will create a graph using the user-defined schema if it does not already exist. If the graph exists, the command will return the existing graph. To overwrite the existing graph, set the drop_existing_graph parameter to True. Note that creating the graph may take several seconds.</p> <pre><code>graph = Graph(\n    graph_schema=graph_schema,\n    tigergraph_connection_config=connection,\n    drop_existing_graph=False,\n)\n</code></pre>"},{"location":"graphrag/msft_graphrag_1/#load-data-to-tigergraph","title":"Load Data to TigerGraph","text":"<p>We will load data into the graph using a pre-defined loading job configuration. The configuration is stored in a YAML file.</p> <pre><code>loading_job_path = resource_dir + \"loading_job_config.yaml\"\ngraph.load_data(loading_job_config=LoadingJobConfig.ensure_config(loading_job_path))\n</code></pre>"},{"location":"graphrag/msft_graphrag_1/#next-steps","title":"Next Steps","text":"<ul> <li>Supporting Microsoft\u2019s GraphRAG: Part 2: Use Jupyter Notebook to explore graph data and perform Graph Analysis.</li> </ul> <p>Start transforming your GraphRAG workflows with the power of TigerGraphX today!</p>"},{"location":"graphrag/msft_graphrag_1_1/","title":"Msft graphrag 1 1","text":"In\u00a0[2]: Copied! <pre>from tigergraphx import Graph, GraphSchema, LoadingJobConfig, TigerGraphConnectionConfig\nresource_dir = \"../../applications/msft_graphrag/query/resources/\"\nschema_path = resource_dir + \"graph_schema.yaml\"\ngraph_schema=GraphSchema.ensure_config(schema_path)\n</pre> from tigergraphx import Graph, GraphSchema, LoadingJobConfig, TigerGraphConnectionConfig resource_dir = \"../../applications/msft_graphrag/query/resources/\" schema_path = resource_dir + \"graph_schema.yaml\" graph_schema=GraphSchema.ensure_config(schema_path) In\u00a0[3]: Copied! <pre>connection = TigerGraphConnectionConfig.ensure_config({\n    \"host\": \"http://127.0.0.1\",\n    \"user_name\": \"tigergraph\",\n    \"password\": \"tigergraph\",\n})\n</pre> connection = TigerGraphConnectionConfig.ensure_config({     \"host\": \"http://127.0.0.1\",     \"user_name\": \"tigergraph\",     \"password\": \"tigergraph\", }) In\u00a0[4]: Copied! <pre>graph = Graph(\n    graph_schema=graph_schema,\n    tigergraph_connection_config=connection,\n    drop_existing_graph=False,\n)\n</pre> graph = Graph(     graph_schema=graph_schema,     tigergraph_connection_config=connection,     drop_existing_graph=False, ) In\u00a0[5]: Copied! <pre>print(graph.number_of_nodes())\n</pre> print(graph.number_of_nodes()) <pre>0\n</pre> <p>After that, we will load data into the graph using a pre-defined loading job configuration. The configuration is stored in a YAML file.</p> In\u00a0[6]: Copied! <pre>loading_job_path = resource_dir + \"loading_job_config.yaml\"\ngraph.load_data(loading_job_config=LoadingJobConfig.ensure_config(loading_job_path))\n</pre> loading_job_path = resource_dir + \"loading_job_config.yaml\" graph.load_data(loading_job_config=LoadingJobConfig.ensure_config(loading_job_path)) <p>Now, let's check the total number of nodes in the graph again. We should observe that some nodes have been successfully loaded into the graph.</p> In\u00a0[7]: Copied! <pre>print(graph.number_of_nodes())\n</pre> print(graph.number_of_nodes()) <pre>808\n</pre>"},{"location":"graphrag/msft_graphrag_1_1/#create-a-graph","title":"Create a Graph\u00b6","text":""},{"location":"graphrag/msft_graphrag_1_1/#define-a-graph-schema","title":"Define a Graph Schema\u00b6","text":"<p>TigerGraph is a schema-based database, which requires defining a schema to structure your graph. This schema specifies the graph name, nodes (vertices), edges (relationships), and their respective attributes.</p> <p>In this example, we will initialize a graph using a schema defined in a YAML file.</p> <p>First, convert the YAML file into a graph schema using the <code>GraphSchema.ensure_config</code> method.</p>"},{"location":"graphrag/msft_graphrag_1_1/#define-the-tigergraph-connection-configuration","title":"Define the TigerGraph Connection Configuration\u00b6","text":"<p>In addition to defining the schema, a connection configuration is necessary to establish communication with the TigerGraph server.</p>"},{"location":"graphrag/msft_graphrag_1_1/#create-a-graph","title":"Create a Graph\u00b6","text":"<p>Running the following command will create a graph using the user-defined schema if it does not already exist. If the graph exists, the command will return the existing graph. To overwrite the existing graph, set the drop_existing_graph parameter to True. Note that creating the graph may take several seconds.</p>"},{"location":"graphrag/msft_graphrag_1_1/#load-data","title":"Load Data\u00b6","text":"<p>First, let's check the total number of nodes in the graph. As anticipated, the graph is currently empty.</p>"},{"location":"graphrag/msft_graphrag_2/","title":"Supporting Microsoft's GraphRAG: Part 2","text":"In\u00a0[2]: Copied! <pre>from tigergraphx import Graph, TigerGraphConnectionConfig\nconnection = TigerGraphConnectionConfig.ensure_config({\n    \"host\": \"http://127.0.0.1\",\n    \"user_name\": \"tigergraph\",\n    \"password\": \"tigergraph\",\n})\nG = Graph.from_db(\"GraphRAG\", connection)\n</pre> from tigergraphx import Graph, TigerGraphConnectionConfig connection = TigerGraphConnectionConfig.ensure_config({     \"host\": \"http://127.0.0.1\",     \"user_name\": \"tigergraph\",     \"password\": \"tigergraph\", }) G = Graph.from_db(\"GraphRAG\", connection) In\u00a0[3]: Copied! <pre>schema = G.get_schema()\nschema[\"graph_name\"]\n</pre> schema = G.get_schema() schema[\"graph_name\"] Out[3]: <pre>'GraphRAG'</pre> In\u00a0[4]: Copied! <pre>for node in schema[\"nodes\"].items():\n    print(node)\n</pre> for node in schema[\"nodes\"].items():     print(node) <pre>('Document', {'primary_key': 'id', 'attributes': {'title': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}, 'id': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}}})\n('TextUnit', {'primary_key': 'id', 'attributes': {'text': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}, 'n_tokens': {'data_type': &lt;DataType.UINT: 'UINT'&gt;, 'default_value': None}, 'id': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}}})\n('Entity', {'primary_key': 'id', 'attributes': {'human_readable_id': {'data_type': &lt;DataType.UINT: 'UINT'&gt;, 'default_value': None}, 'name': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}, 'entity_type': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}, 'description': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}, 'id': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}}})\n('Relationship', {'primary_key': 'id', 'attributes': {'human_readable_id': {'data_type': &lt;DataType.UINT: 'UINT'&gt;, 'default_value': None}, 'rank': {'data_type': &lt;DataType.UINT: 'UINT'&gt;, 'default_value': None}, 'weight': {'data_type': &lt;DataType.DOUBLE: 'DOUBLE'&gt;, 'default_value': None}, 'description': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}, 'id': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}}})\n('Community', {'primary_key': 'id', 'attributes': {'level': {'data_type': &lt;DataType.UINT: 'UINT'&gt;, 'default_value': None}, 'rank': {'data_type': &lt;DataType.DOUBLE: 'DOUBLE'&gt;, 'default_value': None}, 'rank_explanation': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}, 'title': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}, 'full_content': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}, 'summary': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}, 'id': {'data_type': &lt;DataType.STRING: 'STRING'&gt;, 'default_value': None}}})\n</pre> In\u00a0[5]: Copied! <pre>for edge in schema[\"edges\"].items():\n    print(edge)\n</pre> for edge in schema[\"edges\"].items():     print(edge) <pre>('document_contains_text_unit', {'is_directed_edge': False, 'from_node_type': 'Document', 'to_node_type': 'TextUnit', 'attributes': {}})\n('text_unit_contains_entity', {'is_directed_edge': False, 'from_node_type': 'TextUnit', 'to_node_type': 'Entity', 'attributes': {}})\n('text_unit_contains_relationship', {'is_directed_edge': False, 'from_node_type': 'TextUnit', 'to_node_type': 'Relationship', 'attributes': {}})\n('relationship_source', {'is_directed_edge': False, 'from_node_type': 'Relationship', 'to_node_type': 'Entity', 'attributes': {}})\n('relationship_target', {'is_directed_edge': False, 'from_node_type': 'Relationship', 'to_node_type': 'Entity', 'attributes': {}})\n('community_contains_entity', {'is_directed_edge': False, 'from_node_type': 'Community', 'to_node_type': 'Entity', 'attributes': {}})\n('community_contains_relationship', {'is_directed_edge': False, 'from_node_type': 'Community', 'to_node_type': 'Relationship', 'attributes': {}})\n</pre> In\u00a0[6]: Copied! <pre>G.number_of_nodes()\n</pre> G.number_of_nodes() Out[6]: <pre>931</pre> In\u00a0[7]: Copied! <pre>for node_type in schema[\"nodes\"]:\n    print(f\"{node_type}: {G.number_of_nodes(node_type)}\")\n</pre> for node_type in schema[\"nodes\"]:     print(f\"{node_type}: {G.number_of_nodes(node_type)}\") <pre>Document: 1\nTextUnit: 47\nEntity: 398\nRelationship: 426\nCommunity: 59\n</pre> In\u00a0[8]: Copied! <pre>G.number_of_edges()\n</pre> G.number_of_edges() Out[8]: <pre>3630</pre> In\u00a0[9]: Copied! <pre>for edge_type in schema[\"edges\"]:\n    print(f\"{edge_type}: {G.number_of_edges(edge_type)}\")\n</pre> for edge_type in schema[\"edges\"]:     print(f\"{edge_type}: {G.number_of_edges(edge_type)}\") <pre>document_contains_text_unit: 47\ntext_unit_contains_entity: 699\ntext_unit_contains_relationship: 517\nrelationship_source: 426\nrelationship_target: 426\ncommunity_contains_entity: 566\ncommunity_contains_relationship: 949\n</pre> In\u00a0[10]: Copied! <pre>G.get_nodes(node_type=\"Entity\", limit=2)\n</pre> G.get_nodes(node_type=\"Entity\", limit=2) Out[10]: v_id v_type human_readable_id entity_type name description id 0 7373c84a439841d580b4650dac71136f Entity 328 ORGANIZATION HEMERUS CORPORATION Hemerus Corporation is mentioned as one of the... 7373c84a439841d580b4650dac71136f 1 4a5ddbde3f354a79bb5ae3436ab67d25 Entity 294 ORGANIZATION RENALGUARD RenalGuard, developed by RenalGuard Solutions,... 4a5ddbde3f354a79bb5ae3436ab67d25 In\u00a0[11]: Copied! <pre>G.get_nodes(node_type=\"Relationship\", limit=2)\n</pre> G.get_nodes(node_type=\"Relationship\", limit=2) Out[11]: v_id v_type human_readable_id rank weight description id 0 f09d47220cf146a4bc9869976740f230 Relationship 111 122 1 CytoSorb received funding from JPEO-CBD for a ... f09d47220cf146a4bc9869976740f230 1 89d0f67d14504ef19601f6ff0eb2b32c Relationship 7 126 35 CytoSorb, a therapeutic device, has made signi... 89d0f67d14504ef19601f6ff0eb2b32c In\u00a0[12]: Copied! <pre>G.get_nodes(node_type=\"Community\", limit=2)\n</pre> G.get_nodes(node_type=\"Community\", limit=2) Out[12]: v_id v_type summary level full_content rank id rank_explanation title 0 8 Community This report focuses on Baxter and its signific... 0 # Baxter's Innovations in Dialysis and Sepsis ... 8.5 8 The high impact severity rating reflects Baxte... Community 8 1 16 Community The REMOVE study, focusing on the use of CytoS... 0 # REMOVE Study and Its Oversight Community\\n\\n... 7.5 16 The impact severity rating is high due to the ... Community 16"},{"location":"graphrag/msft_graphrag_2/#supporting-microsofts-graphrag-part-2","title":"Supporting Microsoft\u2019s GraphRAG: Part 2\u00b6","text":"<p>In the previous section, we utilized Microsoft's GraphRAG to transform unstructured documents into Parquet files. Using TigerGraphX, we then converted these files into CSV format, created a graph in TigerGraph, and loaded the CSV data into it.</p> <p>Now, let\u2019s use Jupyter Notebook to explore the graph data and perform graph analysis.</p> <p>To run this Jupyter Notebook, you can download the original <code>.ipynb</code> file from msft_graphrag_2.ipynb.</p>"},{"location":"graphrag/msft_graphrag_2/#get-the-graph-from-tigergraph","title":"Get the Graph from TigerGraph\u00b6","text":"<p>Since the graph has already been created in TigerGraph, redefining its schema is unnecessary. Instead, you can provide the graph name to retrieve it. TigerGraphX will verify if the graph exists in TigerGraph and, if it does, will return the corresponding graph.</p>"},{"location":"graphrag/msft_graphrag_2/#display-the-graph-schema","title":"Display the Graph Schema\u00b6","text":"<p>Let's retrieve the graph schema using the <code>get_schema</code> method. The output is a Python dictionary containing three keys: <code>\"graph_name\"</code>, <code>\"nodes\"</code>, and <code>\"edges\"</code>. We'll print each of them individually to explore the schema details.</p>"},{"location":"graphrag/msft_graphrag_2/#retrieve-the-graph-schema-and-display-the-graph-name","title":"Retrieve the Graph Schema and Display the Graph Name\u00b6","text":""},{"location":"graphrag/msft_graphrag_2/#display-the-node-tyeps","title":"Display the Node Tyeps\u00b6","text":""},{"location":"graphrag/msft_graphrag_2/#display-the-edge-types","title":"Display the Edge Types\u00b6","text":""},{"location":"graphrag/msft_graphrag_2/#display-node-and-edge-counts","title":"Display Node and Edge Counts\u00b6","text":"<p>Gain deeper insights into the graph by exploring details such as the total number of nodes and the count of nodes for each node type.</p>"},{"location":"graphrag/msft_graphrag_2/#display-the-total-number-of-nodes","title":"Display the Total Number of Nodes\u00b6","text":""},{"location":"graphrag/msft_graphrag_2/#display-the-count-of-nodes-for-each-node-type","title":"Display the Count of Nodes for Each Node Type\u00b6","text":""},{"location":"graphrag/msft_graphrag_2/#display-the-total-number-of-edges","title":"Display the Total Number of Edges\u00b6","text":""},{"location":"graphrag/msft_graphrag_2/#display-the-count-of-edges-for-each-edge-type","title":"Display the Count of Edges for Each Edge Type\u00b6","text":""},{"location":"graphrag/msft_graphrag_2/#retrieve-sample-nodes-from-the-graph","title":"Retrieve Sample Nodes from the Graph\u00b6","text":""},{"location":"graphrag/msft_graphrag_2/#whats-next","title":"What\u2019s Next?\u00b6","text":"<ul> <li>Supporting Microsoft\u2019s GraphRAG: Part 3: Perform queries using GSQL and Python-native TigerGraphX, with global and local context builders.</li> </ul> <p>Start transforming your GraphRAG workflows with the power of TigerGraphX today!</p>"},{"location":"graphrag/msft_graphrag_3/","title":"Supporting Microsoft's GraphRAG: Part 3","text":"In\u00a0[2]: Copied! <pre>from tigergraphx import Graph, TigerGraphConnectionConfig\nconnection = TigerGraphConnectionConfig.ensure_config({\n    \"host\": \"http://127.0.0.1\",\n    \"user_name\": \"tigergraph\",\n    \"password\": \"tigergraph\",\n})\nG = Graph.from_db(\"GraphRAG\", connection)\n</pre> from tigergraphx import Graph, TigerGraphConnectionConfig connection = TigerGraphConnectionConfig.ensure_config({     \"host\": \"http://127.0.0.1\",     \"user_name\": \"tigergraph\",     \"password\": \"tigergraph\", }) G = Graph.from_db(\"GraphRAG\", connection) In\u00a0[3]: Copied! <pre>G.get_nodes(\n    node_type=\"Entity\",\n    return_attributes=[\"id\", \"name\", \"entity_type\", \"description\"],\n    limit=2,\n)\n</pre> G.get_nodes(     node_type=\"Entity\",     return_attributes=[\"id\", \"name\", \"entity_type\", \"description\"],     limit=2, ) Out[3]: id name entity_type description 0 7373c84a439841d580b4650dac71136f HEMERUS CORPORATION ORGANIZATION Hemerus Corporation is mentioned as one of the... 1 4a5ddbde3f354a79bb5ae3436ab67d25 RENALGUARD ORGANIZATION RenalGuard, developed by RenalGuard Solutions,... In\u00a0[4]: Copied! <pre>start_nodes = [\"7373c84a439841d580b4650dac71136f\", \"4a5ddbde3f354a79bb5ae3436ab67d25\"]\nG.get_neighbors(\n    start_nodes=start_nodes,\n    start_node_type=\"Entity\",\n    edge_types=\"community_contains_entity\",\n    return_attributes=[\"id\", \"title\", \"full_content\"],\n)\n</pre> start_nodes = [\"7373c84a439841d580b4650dac71136f\", \"4a5ddbde3f354a79bb5ae3436ab67d25\"] G.get_neighbors(     start_nodes=start_nodes,     start_node_type=\"Entity\",     edge_types=\"community_contains_entity\",     return_attributes=[\"id\", \"title\", \"full_content\"], ) Out[4]: id title full_content 0 30 Community 30 # CytoSorb and Global Healthcare Impact\\n\\nThi... 1 10 Community 10 # Innovations in Preventing Contrast-Induced N... 2 1 Community 1 # CytoSorb and Its Global Healthcare Impact\\n\\... In\u00a0[5]: Copied! <pre>import tiktoken\nfrom typing import Optional, List\nfrom tigergraphx.graphrag import BaseContextBuilder\nfrom tigergraphx.core import Graph\nclass GlobalContextBuilder(BaseContextBuilder):\n    def __init__(\n        self,\n        graph: Graph,\n        token_encoder: Optional[tiktoken.Encoding] = None,\n    ):\n        \"\"\"Initialize LocalContextBuilder with graph config and token encoder.\"\"\"\n        super().__init__(\n            graph=graph,\n            single_batch=False,\n            token_encoder=token_encoder,\n        )\n    async def build_context(self) -&gt; str | List[str]:\n        \"\"\"Build local context.\"\"\"\n        context: List[str] = []\n        config = {\n            \"max_tokens\": 12000,\n            \"section_name\": \"Communities\",\n            \"return_attributes\": [\"id\", \"rank\", \"title\", \"full_content\"],\n            \"limit\": 1000,\n        }\n        df = self.graph.get_nodes(\n            node_type=\"Community\",\n            return_attributes=config[\"return_attributes\"],\n            limit=config[\"limit\"],\n        )\n        if df is not None:\n            text_context = self.batch_and_convert_to_text(\n                graph_data=df,\n                max_tokens=config[\"max_tokens\"],\n                single_batch=self.single_batch,\n                section_name=config[\"section_name\"],\n            )\n            context.extend(\n                text_context if isinstance(text_context, list) else [text_context]\n            )\n        return context\n</pre> import tiktoken from typing import Optional, List from tigergraphx.graphrag import BaseContextBuilder from tigergraphx.core import Graph class GlobalContextBuilder(BaseContextBuilder):     def __init__(         self,         graph: Graph,         token_encoder: Optional[tiktoken.Encoding] = None,     ):         \"\"\"Initialize LocalContextBuilder with graph config and token encoder.\"\"\"         super().__init__(             graph=graph,             single_batch=False,             token_encoder=token_encoder,         )     async def build_context(self) -&gt; str | List[str]:         \"\"\"Build local context.\"\"\"         context: List[str] = []         config = {             \"max_tokens\": 12000,             \"section_name\": \"Communities\",             \"return_attributes\": [\"id\", \"rank\", \"title\", \"full_content\"],             \"limit\": 1000,         }         df = self.graph.get_nodes(             node_type=\"Community\",             return_attributes=config[\"return_attributes\"],             limit=config[\"limit\"],         )         if df is not None:             text_context = self.batch_and_convert_to_text(                 graph_data=df,                 max_tokens=config[\"max_tokens\"],                 single_batch=self.single_batch,                 section_name=config[\"section_name\"],             )             context.extend(                 text_context if isinstance(text_context, list) else [text_context]             )         return context <p>Here\u2019s how you can utilize the custom global context builder:</p> In\u00a0[15]: Copied! <pre>global_context_builder = GlobalContextBuilder(G)\ncontext_list = await global_context_builder.build_context()\n# Print the first 1000 characters for easier visualization of long text\nprint(context_list[0][:1000])\n</pre> global_context_builder = GlobalContextBuilder(G) context_list = await global_context_builder.build_context() # Print the first 1000 characters for easier visualization of long text print(context_list[0][:1000]) <pre>-----Communities-----\nid|rank|title|full_content\n34|8.5|Community 34|# CytoSorb and FDA: Advancing Cardiac Surgery Innovations\\n\\nThis report focuses on the collaborative efforts between CytoSorb, the U.S. Food and Drug Administration (FDA), and academic institutions in advancing cardiac surgery innovations through the REFRESH I study and its implications for patient care. It highlights the regulatory approvals, the study's publication, and its presentation in prestigious medical forums.\\n\\n## FDA's pivotal role in CytoSorb's cardiac surgery innovations\\n\\nThe U.S. Food and Drug Administration (FDA) has been instrumental in the progression of CytoSorb's cardiac surgery innovations, particularly through the approval of the Investigational Device Exemption (IDE) for the REFRESH I study and subsequent amendments. This approval process underscores the FDA's commitment to facilitating the development of medical devices that can significantly improve patient outcomes. Furthermore, the FDA's \n</pre>"},{"location":"graphrag/msft_graphrag_3/#supporting-microsofts-graphrag-part-3","title":"Supporting Microsoft\u2019s GraphRAG: Part 3\u00b6","text":"<p>To run this Jupyter Notebook, you can download the original <code>.ipynb</code> file from msft_graphrag_3.ipynb.</p>"},{"location":"graphrag/msft_graphrag_3/#get-the-graph-from-tigergraph","title":"Get the Graph from TigerGraph\u00b6","text":""},{"location":"graphrag/msft_graphrag_3/#hybrid-retrieval","title":"Hybrid Retrieval\u00b6","text":"<p>TigerGraph offers two flexible ways to perform hybrid retrieval, allowing you to extract relevant graph and vector data efficiently for GraphRAG workflows.</p>"},{"location":"graphrag/msft_graphrag_3/#using-tigergraphx","title":"Using TigerGraphX\u00b6","text":"<p>TigerGraphX offers an intuitive, Python-native interface for hybrid retrieval, ideal for developers seeking simplicity and ease of use.</p> <p>Key Advantage: Minimal learning curve with high-level Python APIs, seamlessly integrated with existing workflows.</p> <p>Below are some illustrative examples.</p>"},{"location":"graphrag/msft_graphrag_3/#retrieve-nodes-with-specific-attributes","title":"Retrieve Nodes with Specific Attributes\u00b6","text":"<p>You can use the following code to fetch up to two nodes of type \"Entity\" and display their \"id,\" \"entity_type,\" and \"description\" attributes.</p>"},{"location":"graphrag/msft_graphrag_3/#retrieve-neighbors-with-specific-attributes","title":"Retrieve Neighbors with Specific Attributes\u00b6","text":"<p>The following code demonstrates how to fetch neighbors of specific nodes. In this example, the query retrieves neighbors connected to the given <code>start_nodes</code> of type <code>\"Entity\"</code> through the edge type <code>\"community_contains_entity\"</code>. The attributes <code>\"id\"</code>, <code>\"title\"</code>, and <code>\"full_content\"</code> of the neighbors are returned.</p>"},{"location":"graphrag/msft_graphrag_3/#retrieve-top-k-using-tigervectors-vector-search-capability-planned-feature","title":"Retrieve Top-K Using TigerVector's Vector Search Capability [Planned Feature]\u00b6","text":""},{"location":"graphrag/msft_graphrag_3/#using-gsql","title":"Using GSQL\u00b6","text":"<p>For developers seeking fine-grained control or complex retrieval logic, GSQL offers unmatched flexibility. As TigerGraph's built-in query language, GSQL empowers you to perform advanced graph data analysis. For more details, see the official documentation.</p> <p>Key Advantage: Supports complex logic, customization, and direct interaction with TigerGraph\u2019s powerful query engine.</p> <ol> <li>Use an LLM to convert the query into an embedding.</li> <li>Write a GSQL query to retrieve the top-K similar objects and their neighbors, combining structured and vector-based retrieval:</li> </ol> <pre>CREATE OR REPLACE QUERY query (List&lt;float&gt; embedding, int k) {\n  Nodes = TopKVectorSearch({Entity.entity_embedding}, embedding, k);\n\n  Neighbors =\n    SELECT t\n    FROM Nodes:s -(community_contains_entity:e)- :t;\n\n  PRINT Neighbors[Neighbors.id, Neighbors.title, Neighbors.full_content];\n}\n</pre>"},{"location":"graphrag/msft_graphrag_3/#context-building-writing-custom-context-builders","title":"Context Building: Writing Custom Context Builders\u00b6","text":"<p>Context builders play a vital role in graph-powered RAG workflows. They transform retrieved graph data into structured, meaningful contexts for tasks such as interactions with LLMs).</p> <p>TigerGraphX simplifies this process by offering the flexible <code>BaseContextBuilder</code> class, which allows developers to define custom logic for context building.</p>"},{"location":"graphrag/msft_graphrag_3/#key-features-of-basecontextbuilder","title":"Key Features of <code>BaseContextBuilder</code>\u00b6","text":"<p>The <code>BaseContextBuilder</code> class in TigerGraphX provides a strong foundation for creating custom context builders, offering:</p> <ul> <li>Core Abstraction: A reusable framework for building context logic.</li> <li>Customizable Design: Extensibility for implementing both global and query-specific context generation.</li> </ul>"},{"location":"graphrag/msft_graphrag_3/#key-components","title":"Key Components\u00b6","text":"<ol> <li><p>Abstract Method - <code>build_context</code>: Subclasses must implement this method to define the logic for constructing context.</p> <pre>@abstractmethod\nasync def build_context(self, *args, **kwargs) -&gt; str | List[str]:\n    \"\"\"Abstract method to build context.\"\"\"\n    pass\n</pre> </li> <li><p>Batching and Retrieval Methods:</p> <ul> <li><code>batch_and_convert_to_text</code>: Formats graph data into token-aware text.</li> <li><code>retrieve_top_k_objects</code>: Efficiently retrieves top-K objects for query-based context.</li> </ul> </li> </ol>"},{"location":"graphrag/msft_graphrag_3/#example-global-context-builder","title":"Example: Global Context Builder\u00b6","text":""},{"location":"graphrag/msft_graphrag_3/#example-local-context-builder","title":"Example: Local Context Builder\u00b6","text":"<p>To understand the functionality of the <code>LocalContextBuilder</code> class, let's review the key code from its <code>build_context</code> method.</p> <p></p> <pre><code># Retrieve top-k objects\ntop_k_objects: List[str] = await self.retrieve_top_k_objects(query, k=k)\n...\n# Iterate over different neighbor types\nfor neighbor in neighbor_types:\n    df = self.graph.get_neighbors(...)\n    if df is not None:\n        text_context = self.batch_and_convert_to_text(...)\n        context.extend(\n            text_context if isinstance(text_context, list) else [text_context]\n        )\nreturn \"\\n\\n\".join(context)\n</code></pre> <p>For full implementations of different context builders, refer to the following links:</p> <ul> <li>LocalContextBuilder Code</li> </ul> <p>Here\u2019s how you can utilize the custom local context builder:</p> <pre>local_builder = LocalContextBuilder(graph=graph, search_engine=search_engine)\nlocal_context = await local_builder.build_context(query=\"What are the main topics discussed in the article?\")\n</pre>"},{"location":"graphrag/msft_graphrag_3/#integrate-with-llm","title":"Integrate with LLM\u00b6","text":"<p>After successfully building context from TigerGraph, the final step is integrating it with LLMs, including chat models and embedding models.</p> <p>We have provided an example implementation, which you can find here: Example Code.</p>"},{"location":"graphrag/msft_graphrag_3/#workflow-overview","title":"Workflow Overview\u00b6","text":"<p>The integration process follows the workflow illustrated below:</p> <p></p>"},{"location":"graphrag/msft_graphrag_3/#whats-next","title":"What\u2019s Next?\u00b6","text":"<ul> <li>API Reference: Dive deeper into TigerGraphX APIs.</li> </ul> <p>Start transforming your GraphRAG workflows with the power of TigerGraphX today!</p>"},{"location":"reference/features_overview/","title":"Features Overview","text":"<p>TigerGraphX is designed to simplify complex workflows involving graph databases, vector search, and large language models (LLMs). This page provides an overview of the key features of TigerGraphX and serves as a roadmap to the detailed API documentation for each functionality.</p>"},{"location":"reference/features_overview/#1-schema-management-graph-db","title":"1. Schema Management (Graph DB)","text":"<p>TigerGraphX provides intuitive, Python-native APIs for defining and managing graph schemas. This feature includes:</p> <ul> <li>Programmatic creation and modification of vertices and edges.</li> <li>Support for attributes, primary keys, and schema constraints.</li> <li>Compatibility with YAML and JSON schema definitions.</li> </ul>"},{"location":"reference/features_overview/#2-data-loading-management-graph-db","title":"2. Data Loading Management (Graph DB)","text":"<p>Efficiently load data into TigerGraph from a variety of sources. Key capabilities include:</p> <ul> <li>Support for Parquet and CSV files for high-efficiency workflows.</li> <li>Automated loading jobs to streamline the data import process.</li> <li>Data transformation utilities for pre-processing and compatibility.</li> </ul>"},{"location":"reference/features_overview/#3-graph-library-interface-graph-db-graph-query-language","title":"3. Graph Library Interface (Graph DB, Graph Query Language)","text":"<p>Perform common graph operations using Python-native APIs, including:</p> <ul> <li>CRUD operations for vertices and edges.</li> <li>Multi-hop traversals to analyze relationships in the graph.</li> <li>Graph reporting for insights and analysis.</li> </ul>"},{"location":"reference/features_overview/#4-graph-query-interface-graph-db-graph-query-language","title":"4. Graph Query Interface (Graph DB, Graph Query Language)","text":"<p>Execute advanced queries on your TigerGraph database with ease. This feature includes:</p> <ul> <li>Simplified query execution with Python-based methods.</li> <li>Query results formatted as Pandas DataFrames for seamless integration with analytics workflows.</li> </ul>"},{"location":"reference/features_overview/#5-vector-search-capabilities-vector-db","title":"5. Vector Search Capabilities (Vector DB)","text":"<p>Enhance AI-driven applications with vector embedding support. Key functionalities include:</p> <ul> <li>Storing and querying vector embeddings alongside graph data.</li> <li>Top-K similarity searches for retrieving the most relevant entities.</li> <li>Hybrid retrieval workflows combining graph traversal and vector search.</li> </ul>"},{"location":"reference/features_overview/#6-llm-integration-and-support-llm","title":"6. LLM Integration and Support (LLM)","text":"<p>Enable advanced applications by integrating graph data with large language models (LLMs). Key features:</p> <ul> <li>Token-aware context building for LLM workflows.</li> <li>Hybrid retrieval combining graph, semantic, and vector-based searches.</li> <li>Direct API integration with LLM platforms like OpenAI.</li> </ul>"},{"location":"reference/features_overview/#7-configuration-management-graph-db-vector-db-llm","title":"7. Configuration Management (Graph DB, Vector DB, LLM)","text":"<p>Simplify configuration for various integrations and workflows:</p> <ul> <li>Manage graph database settings programmatically.</li> <li>Configure vector search parameters for efficient retrieval.</li> <li>Customize LLM settings for seamless API interaction.</li> </ul>"},{"location":"reference/features_overview/#8-graphrag-support-graph-db-vector-db-llm","title":"8. GraphRAG Support (Graph DB, Vector DB, LLM)","text":"<p>TigerGraphX simplifies Graph-Retrieval Augmented Generation (GraphRAG) workflows. This feature includes:</p> <ul> <li>Schema definition and data preparation for GraphRAG.</li> <li>Hybrid retrieval methods optimized for GraphRAG use cases.</li> <li>Python-native APIs for context building and LLM integration.</li> </ul>"},{"location":"reference/features_overview/#next-steps","title":"Next Steps","text":"<p>To dive deeper into TigerGraphX\u2019s features, explore the detailed API documentation for each section. Start building powerful graph applications today!</p>"}]}